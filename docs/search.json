[
  {
    "objectID": "posts/00. linear regression/2023-08-18-00. Simple Linear Regression.html",
    "href": "posts/00. linear regression/2023-08-18-00. Simple Linear Regression.html",
    "title": "00. Simple Linear Regression",
    "section": "",
    "text": "- 코랩환경에서 아래 순서대로 진행해야 ISLP 패키지 설치시 오류가 발생하지 않는다.\n\n드라이브 마운트\n현재 작업중인 경로로 이동\n패키지 import\n\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n\n\n\ncd /content/drive/MyDrive/Colab Notebooks/ISLP/Linear Regression\n\n/content/drive/MyDrive/Colab Notebooks/ISLP/Linear Regression\n\n\n\n#pip install ISLP\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom ISLP import load_data\n\nimport statsmodels.api as sm\n\nfrom statsmodels.stats.outliers_influence \\\nimport variance_inflation_factor as VIF\nfrom statsmodels.stats.anova import anova_lm\nfrom ISLP.models import (ModelSpec as MS,\nsummarize,poly)"
  },
  {
    "objectID": "posts/00. linear regression/2023-08-18-00. Simple Linear Regression.html#표현1",
    "href": "posts/00. linear regression/2023-08-18-00. Simple Linear Regression.html#표현1",
    "title": "00. Simple Linear Regression",
    "section": "표현1",
    "text": "표현1\n\\[\\hat {y} = \\hat {\\beta}_1x + \\hat {\\beta}_0\\]"
  },
  {
    "objectID": "posts/00. linear regression/2023-08-18-00. Simple Linear Regression.html#표현-2-starstarstar",
    "href": "posts/00. linear regression/2023-08-18-00. Simple Linear Regression.html#표현-2-starstarstar",
    "title": "00. Simple Linear Regression",
    "section": "표현 2 (\\(\\star\\star\\star\\))",
    "text": "표현 2 (\\(\\star\\star\\star\\))\n- 표현 2가 자주 쓰이니 잘 알아두자.\n\\[\\bf \\hat Y = X \\hat {\\boldsymbol{\\beta}} = \\begin{bmatrix}\n1  & x_1 \\\\ 1 & x_2  \\\\ \\dots & \\dots \\\\  1  & x_n\\end{bmatrix}\\begin{bmatrix} \\hat \\beta_0  \\\\ \\hat \\beta_1 \\end{bmatrix} \\]"
  },
  {
    "objectID": "posts/00. linear regression/2023-08-18-00. Simple Linear Regression.html#데이터-로드",
    "href": "posts/00. linear regression/2023-08-18-00. Simple Linear Regression.html#데이터-로드",
    "title": "00. Simple Linear Regression",
    "section": "데이터 로드",
    "text": "데이터 로드\n- 아래의 데이터를 살펴보자.\n\nBoston data : 보스턴 시의 주택 가격에 대한 데이터 정보\n\ncrim : 자치시 별 1인당 범죄율\nzn : 25,000 평방피트르 초과하는 거주지역의 비율\nindus : 비소매상업지역이 점유하고 있는 토지의 비율\nchas : 찰스강에 대한 더미변수 (강의 경계에 위치한 경우는 1, 아니면 0)\nnos : 10ppm당 농축 일산화 질소\nrm : 주택 1가구당 평균 방의 개수\nage : 1940년 이전에 건축된 소유주택의 비율\ndis : 5개의 보스턴 직업센터까지의 접근성 지수\nrad : 방사형 도로까지의 접근성 지수\ntax : 10,000 달러 당 재산 세율\nptratio : 자치시별 학생/교사 비율\nlstat : 모집단의 하위 계층의 비율\nmedv : 본인 소유의 주택가격(중앙값) (단위 : $ 1,000)\n\n\n\nBoston = load_data(\"Boston\")\nBoston.columns\n#len(Boston.columns)\n\nIndex(['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax',\n       'ptratio', 'lstat', 'medv'],\n      dtype='object')"
  },
  {
    "objectID": "posts/00. linear regression/2023-08-18-00. Simple Linear Regression.html#xy-생성",
    "href": "posts/00. linear regression/2023-08-18-00. Simple Linear Regression.html#xy-생성",
    "title": "00. Simple Linear Regression",
    "section": "X,y 생성",
    "text": "X,y 생성\n\\[\\bf X = \\begin{bmatrix}\n1  & x_1 \\\\ 1 & x_2  \\\\ \\dots & \\dots \\\\  1  & x_n\\end{bmatrix}\\]\n\nlstat 변수를 예측변수로, medv변수를 반응변수로 사용\n\n\nX = pd.DataFrame({\"intercept\" : np.ones(Boston.shape[0]),\n                  \"lstat\" : Boston[\"lstat\"]})\ny = Boston[\"medv\"]\nX.head()\n\n\n  \n    \n\n\n\n\n\n\nintercept\nlstat\n\n\n\n\n0\n1.0\n4.98\n\n\n1\n1.0\n9.14\n\n\n2\n1.0\n4.03\n\n\n3\n1.0\n2.94\n\n\n4\n1.0\n5.33\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- OLS : Ordinary Least Squares의 약자로, 주어진 데이터에서 오차의 제곱을 최소화하는 \\(\\beta_i\\)를 추정한다."
  },
  {
    "objectID": "posts/00. linear regression/2023-08-18-00. Simple Linear Regression.html#모델-적합",
    "href": "posts/00. linear regression/2023-08-18-00. Simple Linear Regression.html#모델-적합",
    "title": "00. Simple Linear Regression",
    "section": "모델 적합",
    "text": "모델 적합\n\nmodel = sm.OLS(y,X)\nresults = model.fit()\n\n\ns_result = summarize(results)\n\n\ns_result\n\n\n  \n    \n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n\n\n\n\nintercept\n34.5538\n0.563\n61.415\n0.0\n\n\nlstat\n-0.9500\n0.039\n-24.528\n0.0\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- 적합된 모델을 해석하면 다음과 같다.\n\\[\\hat {\\text{medv}} = -0.95 \\times \\text{lstat} + 34\n.5538\\]"
  },
  {
    "objectID": "posts/00. linear regression/2023-08-18-00. Simple Linear Regression.html#시각화",
    "href": "posts/00. linear regression/2023-08-18-00. Simple Linear Regression.html#시각화",
    "title": "00. Simple Linear Regression",
    "section": "시각화",
    "text": "시각화\n\nx = Boston[\"lstat\"]\nyhat = results.predict()\n\n\n\nCode\nplt.plot(x,y,\"o\",label = r\"$(x,y)$\",alpha=0.3)\nplt.plot(x,yhat,\"--\",label = r\"$(x,\\hat {y})$\")\nplt.legend()\n\n\n&lt;matplotlib.legend.Legend at 0x79df6f668a90&gt;"
  },
  {
    "objectID": "posts/00. linear regression/2023-08-18-00. Simple Linear Regression.html#modelspec-bf-x를-선언하는-또-다른-방법",
    "href": "posts/00. linear regression/2023-08-18-00. Simple Linear Regression.html#modelspec-bf-x를-선언하는-또-다른-방법",
    "title": "00. Simple Linear Regression",
    "section": "ModelSpec() : \\(\\bf X\\)를 선언하는 또 다른 방법",
    "text": "ModelSpec() : \\(\\bf X\\)를 선언하는 또 다른 방법\n- 우리는 앞서 다음과 같은 module을 import 했다\nfrom ISLP.models import (ModelSpec as MS,\nsummarize,poly)\n- ModelSpec이라는 모듈을 MS로 사용할 것으로 지칭\n- 이 모듈은 예측변수 \\(x\\)를 컴퓨터가 이해할 수 있게끔 변환해준다.\n- step1. 전달할 \\(x\\)를 다음과 같이 전달\n\ndesign = MS(['lstat'])\n\n- step2. 컴퓨터가 이해할 수 있는 형태에 맞게 \\(x\\)를 변환 \\(\\to\\) 주어진 \\(x\\)를 매트릭스 형태로 변환해줌\n\nX = design.fit_transform(Boston)\n\n\nX.head()\n\n\n  \n    \n\n\n\n\n\n\nintercept\nlstat\n\n\n\n\n0\n1.0\n4.98\n\n\n1\n1.0\n9.14\n\n\n2\n1.0\n4.03\n\n\n3\n1.0\n2.94\n\n\n4\n1.0\n5.33\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n\n모델 적합\n\nmodel = sm.OLS(y, X)\nresults = model.fit()\n\n\n\n모델 요약\n\nreport = results.summary()\n\n\nreport.tables[0]\n\n\nOLS Regression Results\n\n\nDep. Variable:\nmedv\nR-squared:\n0.544\n\n\nModel:\nOLS\nAdj. R-squared:\n0.543\n\n\nMethod:\nLeast Squares\nF-statistic:\n601.6\n\n\nDate:\nSat, 19 Aug 2023\nProb (F-statistic):\n5.08e-88\n\n\nTime:\n04:11:50\nLog-Likelihood:\n-1641.5\n\n\nNo. Observations:\n506\nAIC:\n3287.\n\n\nDf Residuals:\n504\nBIC:\n3295.\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n- 이런식으로 위의 표에 대한 데이터도 확인할 수 있음\n\nreport.tables[0].data\n\n[['Dep. Variable:', 'medv', '  R-squared:         ', '   0.544'],\n ['Model:', 'OLS', '  Adj. R-squared:    ', '   0.543'],\n ['Method:', 'Least Squares', '  F-statistic:       ', '   601.6'],\n ['Date:', 'Sat, 19 Aug 2023', '  Prob (F-statistic):', '5.08e-88'],\n ['Time:', '04:11:50', '  Log-Likelihood:    ', ' -1641.5'],\n ['No. Observations:', '   506', '  AIC:               ', '   3287.'],\n ['Df Residuals:', '   504', '  BIC:               ', '   3295.'],\n ['Df Model:', '     1', '                     ', ' '],\n ['Covariance Type:', 'nonrobust', '                     ', ' ']]\n\n\n- 추정된 회귀계수값 확인\n\nresults.params\n\nintercept    34.553841\nlstat        -0.950049\ndtype: float64\n\n\n\n\n모델 해석1 (모델의 유의성)\n\nreport.tables[0]\n\n\nOLS Regression Results\n\n\nDep. Variable:\nmedv\nR-squared:\n0.544\n\n\nModel:\nOLS\nAdj. R-squared:\n0.543\n\n\nMethod:\nLeast Squares\nF-statistic:\n601.6\n\n\nDate:\nSat, 19 Aug 2023\nProb (F-statistic):\n5.08e-88\n\n\nTime:\n04:11:50\nLog-Likelihood:\n-1641.5\n\n\nNo. Observations:\n506\nAIC:\n3287.\n\n\nDf Residuals:\n504\nBIC:\n3295.\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n- 결정계수 (\\(R^2\\)) 값을 살펴보니 약 54%의 설명력을 가진 모델이다.\n- 또한, F통계량에 근거한 p-value 값을 살펴보았을 때 주어진 표본으로 부터 추출된 모형은 통계적으로 유의하다.\n\n\n모델 해석2 (예측변수의 유의성)\n\nreport.tables[1]\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nintercept\n34.5538\n0.563\n61.415\n0.000\n33.448\n35.659\n\n\nlstat\n-0.9500\n0.039\n-24.528\n0.000\n-1.026\n-0.874\n\n\n\n\n\n- lstat의 coef \\(\\to\\) 즉, 회귀계수에 대한 유의성 검정결과\n\np-value값을 살펴본 결과 유의수준 0.05에서 유의성을 만족한다. 따라서 lstat의 회귀계수는 통계적으로 유의하다.\n\n\n\n시각화\n\nyhat = results.predict()\nx = X[\"lstat\"]\n\n- 적합된 \\((x,\\hat y)\\)와 \\((x,y)\\)를 시각화한 결과 몇몇 이상치를 제외하고 잘 예측하고 있는것 같다.\n\n\nCode\nplt.plot(x,y,\"o\",label = r\"$(x,y)$\",alpha=0.3)\nplt.plot(x,yhat,\"--\",label = r\"$(x,\\hat {y})$\")\nplt.legend()\n\n\n&lt;matplotlib.legend.Legend at 0x79df6f514ee0&gt;"
  },
  {
    "objectID": "posts/00. linear regression/2023-08-18-00. Simple Linear Regression.html#test-data에-대한-예측",
    "href": "posts/00. linear regression/2023-08-18-00. Simple Linear Regression.html#test-data에-대한-예측",
    "title": "00. Simple Linear Regression",
    "section": "test data에 대한 예측",
    "text": "test data에 대한 예측\n- 임의의 데이터 생성\n\nnew_df = pd.DataFrame({'lstat':[5, 10, 15]})\nnewX = design.fit_transform(new_df)\nnewX\n\n\n  \n    \n\n\n\n\n\n\nintercept\nlstat\n\n\n\n\n0\n1.0\n5\n\n\n1\n1.0\n10\n\n\n2\n1.0\n15\n\n\n\n\n\n\n    \n\n  \n    \n\n  \n    \n  \n    \n\n  \n\n    \n  \n\n\n\n  \n\n\n    \n        \n    \n\n  \n\n\n\n  \n\n    \n  \n\n\n- 예측\n\nnew_predictions = results.get_prediction(newX)\nnew_predictions.predicted_mean ## 예측값 확인\n\narray([29.80359411, 25.05334734, 20.30310057])\n\n\n- 예측값에 대한 신뢰구간 (유의수준 = 0.05)\n\nnew_predictions.conf_int(alpha=0.05)\n\narray([[29.00741194, 30.59977628],\n       [24.47413202, 25.63256267],\n       [19.73158815, 20.87461299]])\n\n\n- 예측값에 대한 예측구간 (유의수준 = 0.05)\n\nnew_predictions.conf_int(obs=True, alpha=0.05)\n\narray([[17.56567478, 42.04151344],\n       [12.82762635, 37.27906833],\n       [ 8.0777421 , 32.52845905]])"
  },
  {
    "objectID": "posts/00. linear regression/2023-08-18-00. Simple Linear Regression.html#fitted-value-vs-residual",
    "href": "posts/00. linear regression/2023-08-18-00. Simple Linear Regression.html#fitted-value-vs-residual",
    "title": "00. Simple Linear Regression",
    "section": "fitted value vs residual",
    "text": "fitted value vs residual\n\\[\\text{resid} = y-\\hat y\\]\n\nfv = results.fittedvalues ## 적합치\nre = results.resid ## 잔차\n\n\nfrom scipy import stats\n\n\nfig,axes = plt.subplots(1,2,figsize=(12,6))\n\nax1,ax2 = axes\nax1.plot(fv,re,\"o\",alpha=0.3)\nax1.axhline(0,c=\"k\",ls =\"--\") ## c=k 검은색, ls= linestyle\nax1.set_xlabel(\"fitted value\")\nax1.set_ylabel(\"Residual\")\nax1.set_title(\"fitted value vs Residual\")\n\n_ = stats.probplot(re,plot=ax2)\nax2.set_ylabel(\"standard residual\")\nax2.set_title(\"qqplot\")\n\nText(0.5, 1.0, 'qqplot')\n\n\n\n\n\n\n잔차 해석\n\n잔차 plot을 살펴본 결과 : fitted value가 커질수록 잔차의 분산도 커지고 있다. \\(\\to\\) 잔차가 등분산성을 만족하지 못하고 있다.\n\n반응변수에 변환을 취하거나, 가중최소 제곱법을 이용해 해결해야한다.\n\nqqplot을 그려본결과 왼쪽 꼬리는 이론적으로 나와야할 값보다. 큰값을 가진다.\n오른쪽 꼬리는 마찬가지이다.\n즉, 잔차는 정규성 가정에 위배되는 것처럼 보인다."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n- 전북대학교 통계학과 학사(부전공: 컴퓨터공학) 졸업 | 3.67 / 4.50 | 2015. 03 ~ 2021. 02\n- 전북대학교 통계학과 석사 졸업 | 4.44 / 4.50 | 2021. 03 ~ 2023. 02"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\n- 국민연금공단 빅데이터부 현장실습 | 2020. 03 ~ 2020. 06\n- 지역 문화산업 융복합 데이터 전문가 과정 | 과학기술정보통신부, 한국데이터산업진흥원 | 2021. 06 ~ 2021. 08\n- 빅데이터 혁신공유대학사업 서포터즈 |전북대학교 빅데이터 현신공유대학사업| 2021. 07. 01 ~ 2021. 10. 31\n- KT AIVLE School DX Consultant Track | KT | 2023. 08 ~"
  },
  {
    "objectID": "about.html#publications",
    "href": "about.html#publications",
    "title": "About Me",
    "section": "Publications",
    "text": "Publications\n- 데이터 분석을 통한 지역별 고령친화도 시각화\n`-` 김영선, 강민구, 이강철 등  | 문화융복합아카이빙연구소 | 2021. 10 | 기록관리/보존 \n- 핵심어 추출 및 데이터 증강기법을 이용한 텍스트 분류 모델 성능 개선\n`-` 이강철, 안정용 | 한국자료분석학회 | 한국자료분석학회 | 2022. 10 | 통계학"
  },
  {
    "objectID": "about.html#certificate",
    "href": "about.html#certificate",
    "title": "About Me",
    "section": "Certificate",
    "text": "Certificate\n- 워드프로세서 | 대한상공회의소 | 19-19-017981 | 2019. 08. 30\n- 데이터분석준전문가(ADsP) | 한국데이터진흥원 | ADsP-0223898 | 2019. 10. 01\n- 사회조사분석사 2급 | 한국산업인력공단 | 19201142418N | 2019. 10. 01"
  },
  {
    "objectID": "about.html#conctact",
    "href": "about.html#conctact",
    "title": "About Me",
    "section": "Conctact",
    "text": "Conctact\n- rkdcjf8232@gmail.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISLP2023",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nAug 18, 2023\n\n\n00. Simple Linear Regression\n\n\nGC \n\n\n\n\n\n\nNo matching items"
  }
]