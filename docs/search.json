[
  {
    "objectID": "posts/study/2023-09-12-01. tesorflow (1) .html",
    "href": "posts/study/2023-09-12-01. tesorflow (1) .html",
    "title": "01. tensorflow (1)",
    "section": "",
    "text": "강의노트\n\nyoutube: https://youtube.com/playlist?list=PLQqh36zP38-z8oR8bQZHR0mpy_9OcsWOz\n\n\n\nTensorFlow 란?\n\n-&gt; 딥러닝 및 기계 학습을 위한 오픈소스 딥러닝 프레임워크 중 하나\n\nCNN, RNN 등 다양한 딥러닝 모델 구현 도구 포함\n텐서보드 지원(시각화)\n분산 컴퓨팅 지원 및 확장성 : GPU 등 하드웨어 가속 지원\n자동 미분 기능 : 그래디언드(기울기)를 자동으로 계산하여 역전파 알고리즘을 구현 지원 등\n\n\n\nTensor\n-&gt; 다차원 배열로, 수치데이터를 저장하고 다루는데 사용함. 보통 백터, 행렬, n차원 배열 등의 형태를 가지며, 데이터의 종류에 따라 다양한 유형이 존재함 1. 머신러닝 및 딥러닝 데이터가 주로 텐서로 표현되고(이미지, 텍스트, 오디오 등), 2. 신경망의 학습 및 추론 과정에서 가중치와 편향을 조정하고 데이터를 변화하는데 텐서 연산을 기반으로 구성\n\n\n\n\nTensorFlow 설치하기(참고) : https://brunch.co.kr/@mapthecity/15\n\n\n\n\nimport\n\nimport tensorflow as tf\nimport numpy as np\n\n\ntf.config.experimental.list_physical_devices('GPU')\n\n[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n\n\n\n# physical_device:GPU:0 =&gt; TensorFlow가 인식한 GPU들 중에서 첫번째 GPU를 사용한다는 뜻\n\n\ntf.__version__\n\n'2.10.0'\n\n\n\n\ntf.constant()\n\n파이썬의 list나 numpy 배열과 같은 데이터를 받아 Tensor로 변환하는 함수\n\n\n예비학습: 중첩리스트\n- 리스트\n\nlst = [1,2,4,5,6]\nlst \n\n[1, 2, 4, 5, 6]\n\n\n\nlst[1] # 두번쨰원소 \n\n2\n\n\n\nlst[-1] # 마지막원소 \n\n6\n\n\n- (2,2) matrix 느낌의 list\n\nlst= [[1,2],[3,4]]\nlst\n\n[[1, 2], [3, 4]]\n\n\n위를 아래와 같은 매트릭스로 생각할수 있다.\n1 2 \n3 4 \n\nprint(lst[0][0]) # (1,1) \nprint(lst[0][1]) # (1,2) \nprint(lst[1][0]) # (2,1) \nprint(lst[1][1]) # (2,2) \n\n1\n2\n3\n4\n\n\n- (4,1) matrix 느낌의 list\n\nlst=[[1],[2],[3],[4]] # (4,1) matrix = 길이가 4인 col-vector\nlst\n\n[[1], [2], [3], [4]]\n\n\n- (1,4) matrix 느낌의 list\n\nlst=[[1,2,3,4]] # (1,4) matrix = 길이가 4인 row-vector \nlst\n\n[[1, 2, 3, 4]]\n\n\n\n\n선언\n- 스칼라(0차원)\n\ntf.constant(3.14)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=3.14&gt;\n\n\n\ntf.constant(3.14)+tf.constant(3.14)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=6.28&gt;\n\n\n- 벡터(1차원)\n\n_vector=tf.constant([1,2,3])\n\n\n_vector[-1]\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=3&gt;\n\n\n- 매트릭스 즉, 행렬(2차원)\n\n_matrix= tf.constant([[1,0],[0,1]])\n_matrix\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 0],\n       [0, 1]], dtype=int32)&gt;\n\n\n- array\n\ntf.constant([[[0,1,1],[1,2,-1]],[[0,1,2],[1,2,-1]]])\n\n&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[ 0,  1,  1],\n        [ 1,  2, -1]],\n\n       [[ 0,  1,  2],\n        [ 1,  2, -1]]], dtype=int32)&gt;\n\n\n\n\n인덱싱 : 파이썬 동일\n\ntype(tf.constant(3.14))\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\n_matrix = tf.constant([[1,2],[3,4]])\n_matrix\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4]], dtype=int32)&gt;\n\n\n\n_matrix[0][0]\n\n&lt;tf.Tensor: shape=(), dtype=int32, numpy=1&gt;\n\n\n\n_matrix[0]\n\n&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt;\n\n\n\n_matrix[0,:] # 행, 열 순서\n\n&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)&gt;\n\n\n\n_matrix[:,0]\n\n&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 3], dtype=int32)&gt;\n\n\n\n\ntf.constant는 불편하다.\n- 불편한점 1. 모든 원소가 같은 dtype을 가지고 있어야함. 2. 원소 수정이 불가능함. 3. 묵시적 형변환이 불가능하다.\n- 원소수정이 불가능함\n\na=tf.constant([1,22,33])\na\n\n&lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([ 1, 22, 33], dtype=int32)&gt;\n\n\n\na[0]=11\n\nTypeError: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment\n\n\n- 묵시적 형변환이 불가능하다\n\ntf.constant(1)+tf.constant(3.14)\n\nInvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:AddV2]\n\n\n\ntf.constant(1.0)+tf.constant(3.14)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=4.1400003&gt;\n\n\n\n# cast 이용(Tensor의 형변환) : tf.constant(1.0)+tf.constant(3.14)\na1 = tf.constant(1.0)\nb1 = tf.constant(3.14)\nd1 = tf.cast(b1, tf.float32)\nprint(d1)\nprint(tf.add(a1,d1))\n\ntf.Tensor(3.14, shape=(), dtype=float32)\ntf.Tensor(4.1400003, shape=(), dtype=float32)\n\n\n- 같은 float도 안되는 경우가 있음\n\ntf.constant(1.0,dtype=tf.float64)\n\n&lt;tf.Tensor: shape=(), dtype=float64, numpy=1.0&gt;\n\n\n\ntf.constant(3.14)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=3.14&gt;\n\n\n\ntf.constant(1.0,dtype=tf.float64)+tf.constant(3.14)\n\nInvalidArgumentError: cannot compute AddV2 as input #1(zero-based) was expected to be a double tensor but is a float tensor [Op:AddV2]\n\n\n\n\ntf.constant \\(\\to\\) 넘파이\n\nnp.array(tf.constant(1)) # 방법1\n\narray(1, dtype=int32)\n\n\n\na=tf.constant([3.14,-3.14])\ntype(a)\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\na.numpy()\n\narray([ 3.14, -3.14], dtype=float32)\n\n\n\n\n연산\n- 더하기\n\na=tf.constant([1,2])\nb=tf.constant([3,4])\na+b\n\n&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)&gt;\n\n\n\ntf.add(a,b)\n\n&lt;tf.Tensor: shape=(2,), dtype=int32, numpy=array([4, 6], dtype=int32)&gt;\n\n\n- 곱하기\n\na=tf.constant([[1,2],[3,4]])\nb=tf.constant([[5,6],[7,8]])\na*b\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[ 5, 12],\n       [21, 32]], dtype=int32)&gt;\n\n\n\ntf.multiply(a,b)\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[ 5, 12],\n       [21, 32]], dtype=int32)&gt;\n\n\n- 매트릭스의 곱(2차원에서만 작동)\n\na=tf.constant([[1,0],[0,1]]) # (2,2)\nb=tf.constant([[5],[7]]) # (2,1) \na@b\n\n&lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy=\narray([[5],\n       [7]], dtype=int32)&gt;\n\n\n\ntf.matmul(a,b)\n\n&lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy=\narray([[5],\n       [7]], dtype=int32)&gt;\n\n\n- 역행렬\n\n\n\n\na=tf.constant([[1,0],[0,2]])\na\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 0],\n       [0, 2]], dtype=int32)&gt;\n\n\n\ntf.linalg.inv(a)\n\nInvalidArgumentError: Value for attr 'T' of int32 is not in the list of allowed values: double, float, half, complex64, complex128\n    ; NodeDef: {{node MatrixInverse}}; Op&lt;name=MatrixInverse; signature=input:T -&gt; output:T; attr=adjoint:bool,default=false; attr=T:type,allowed=[DT_DOUBLE, DT_FLOAT, DT_HALF, DT_COMPLEX64, DT_COMPLEX128]&gt; [Op:MatrixInverse]\n\n\n\na=tf.constant([[1.0,0.0],[0.0,2.0]])\ntf.linalg.inv(a)\n\n&lt;tf.Tensor: shape=(2, 2), dtype=float32, numpy=\narray([[1. , 0. ],\n       [0. , 0.5]], dtype=float32)&gt;\n\n\n- tf.linalg. + tab을 누르면 좋아보이는 연산들 많음\n\na=tf.constant([[1.0,2.0],[3.0,4.0]])\nprint(a)\ntf.linalg.det(a)\n\ntf.Tensor(\n[[1. 2.]\n [3. 4.]], shape=(2, 2), dtype=float32)\n\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=-2.0&gt;\n\n\n\n?tf.linalg.det  # 행렬의 특성 설명\n\n\nSignature: tf.linalg.det(input, name=None)\nDocstring:\nComputes the determinant of one or more square matrices.\nThe input is a tensor of shape `[..., M, M]` whose inner-most 2 dimensions\nform square matrices. The output is a tensor containing the determinants\nfor all input submatrices `[..., :, :]`.\nArgs:\n  input: A `Tensor`. Must be one of the following types: `half`, `float32`, `float64`, `complex64`, `complex128`.\n    Shape is `[..., M, M]`.\n  name: A name for the operation (optional).\nReturns:\n  A `Tensor`. Has the same type as `input`.\nFile:      c:\\users\\user\\anaconda3\\envs\\dx_env\\lib\\site-packages\\tensorflow\\python\\ops\\gen_linalg_ops.py\nType:      function\n\n\n\n\n\ntf.linalg.trace(a)\n\n&lt;tf.Tensor: shape=(), dtype=float32, numpy=5.0&gt;\n\n\n::: {.cell jupyter=‘{“outputs_hidden”:true}’ tags=‘[]’ execution_count=15}\n?tf.linalg.trace # 대각선 상의 원소의 합\n::: {.cell-output .cell-output-display}\n\nSignature: tf.linalg.trace(x, name=None)\nDocstring:\nCompute the trace of a tensor `x`.\n`trace(x)` returns the sum along the main diagonal of each inner-most matrix\nin x. If x is of rank `k` with shape `[I, J, K, ..., L, M, N]`, then output\nis a tensor of rank `k-2` with dimensions `[I, J, K, ..., L]` where\n`output[i, j, k, ..., l] = trace(x[i, j, k, ..., l, :, :])`\nFor example:\n```python\nx = tf.constant([[1, 2], [3, 4]])\ntf.linalg.trace(x)  # 5\nx = tf.constant([[1, 2, 3],\n                 [4, 5, 6],\n                 [7, 8, 9]])\ntf.linalg.trace(x)  # 15\nx = tf.constant([[[1, 2, 3],\n                  [4, 5, 6],\n                  [7, 8, 9]],\n                 [[-1, -2, -3],\n                  [-4, -5, -6],\n                  [-7, -8, -9]]])\ntf.linalg.trace(x)  # [15, -15]\nArgs: x: tensor. name: A name for the operation (optional). Returns: The trace of input tensor. File: c:_env-packages_ops.py Type: function\n\n:::\n\n:::\n:::\n\n\n#### 형태변환\n\n`-` 기본: tf.reshape() 를 이용\n\n::: {.cell execution_count=44}\n``` {.python .cell-code}\na=tf.constant([1,2,3,4])\na\n\n&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;\n\n\n\ntf.reshape(a,(4,1))\n\n&lt;tf.Tensor: shape=(4, 1), dtype=int32, numpy=\narray([[1],\n       [2],\n       [3],\n       [4]], dtype=int32)&gt;\n\n\n\ntf.reshape(a,(2,2))\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4]], dtype=int32)&gt;\n\n\n\ntf.reshape(a,(2,2,1))\n\n&lt;tf.Tensor: shape=(2, 2, 1), dtype=int32, numpy=\narray([[[1],\n        [2]],\n\n       [[3],\n        [4]]], dtype=int32)&gt;\n\n\n- 다차원\n\na=tf.constant([1,2,3,4,5,6,7,8,9,10,11,12])\na\n\n&lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)&gt;\n\n\n\ntf.reshape(a,(2,2,3))\n\n&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]], dtype=int32)&gt;\n\n\n\ntf.reshape(a,(4,3))\n\n&lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy=\narray([[ 1,  2,  3],\n       [ 4,  5,  6],\n       [ 7,  8,  9],\n       [10, 11, 12]], dtype=int32)&gt;\n\n\n\na=tf.constant([1,2,3,4,5,6,7,8,9,10,11,12])\na\n\n&lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)&gt;\n\n\n\ntf.reshape(a,(4,-1)) # -1 이면 전체 크기가 일정하게 유지되도록 해당 차원의 크기가 계산됨\n\n&lt;tf.Tensor: shape=(4, 3), dtype=int32, numpy=\narray([[ 1,  2,  3],\n       [ 4,  5,  6],\n       [ 7,  8,  9],\n       [10, 11, 12]], dtype=int32)&gt;\n\n\n\ntf.reshape(a,(2,2,-1))\n\n&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]], dtype=int32)&gt;\n\n\n\nb=tf.reshape(a,(2,2,-1))\nb\n\n&lt;tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=\narray([[[ 1,  2,  3],\n        [ 4,  5,  6]],\n\n       [[ 7,  8,  9],\n        [10, 11, 12]]], dtype=int32)&gt;\n\n\n\ntf.reshape(b,-1)\n\n&lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12], dtype=int32)&gt;\n\n\n\n\n선언고급\n- 다른 자료형 (리스트나 넘파이)로 만들고 바꾸는것도 좋다.\n\nnp.diag([1,2,3,4]) # np.diag : 대각 행렬 생성\n\narray([[1, 0, 0, 0],\n       [0, 2, 0, 0],\n       [0, 0, 3, 0],\n       [0, 0, 0, 4]])\n\n\n\ntf.constant(np.diag([1,2,3,4]))\n\n&lt;tf.Tensor: shape=(4, 4), dtype=int64, numpy=\narray([[1, 0, 0, 0],\n       [0, 2, 0, 0],\n       [0, 0, 3, 0],\n       [0, 0, 0, 4]])&gt;\n\n\n- tf.ones, tf.zeros\n\ntf.zeros([3,3])\n\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[0., 0., 0.],\n       [0., 0., 0.],\n       [0., 0., 0.]], dtype=float32)&gt;\n\n\n\ntf.reshape(tf.constant([0]*9),(3,3))\n\n&lt;tf.Tensor: shape=(3, 3), dtype=int32, numpy=\narray([[0, 0, 0],\n       [0, 0, 0],\n       [0, 0, 0]], dtype=int32)&gt;\n\n\n- range(10)\n\na=range(0,12)\ntf.constant(a)\n\n&lt;tf.Tensor: shape=(12,), dtype=int32, numpy=array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11], dtype=int32)&gt;\n\n\n\ntf.constant(range(1,20,3)) \n\n&lt;tf.Tensor: shape=(7,), dtype=int32, numpy=array([ 1,  4,  7, 10, 13, 16, 19], dtype=int32)&gt;\n\n\n- tf.linspace\n\ntf.linspace(0,1,10)\n\n&lt;tf.Tensor: shape=(10,), dtype=float64, numpy=\narray([0.        , 0.11111111, 0.22222222, 0.33333333, 0.44444444,\n       0.55555556, 0.66666667, 0.77777778, 0.88888889, 1.        ])&gt;\n\n\n\n\ntf.concat\n- (2,1) concat (2,1) =&gt; (2,2) - 두번째 축이 바뀌었다. =&gt; axis=1\n\na=tf.constant([[1],[2]])\nb=tf.constant([[3],[4]])\na,b\n\n(&lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n array([[1],\n        [2]], dtype=int32)&gt;,\n &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n array([[3],\n        [4]], dtype=int32)&gt;)\n\n\n\ntf.concat([a,b],axis=1)\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 3],\n       [2, 4]], dtype=int32)&gt;\n\n\n- (2,1) concat (2,1) =&gt; (4,1) - 첫번째 축이 바뀌었다. =&gt; axis=0\n\na=tf.constant([[1],[2]])\nb=tf.constant([[3],[4]])\na,b\n\n(&lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n array([[1],\n        [2]], dtype=int32)&gt;,\n &lt;tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n array([[3],\n        [4]], dtype=int32)&gt;)\n\n\n\ntf.concat([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(4, 1), dtype=int32, numpy=\narray([[1],\n       [2],\n       [3],\n       [4]], dtype=int32)&gt;\n\n\n- (1,2) concat (1,2) =&gt; (2,2) - 첫번째 // axis=0\n\na=tf.constant([[1,2]])\nb=tf.constant([[3,4]])\n\n\ntf.concat([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1, 2],\n       [3, 4]], dtype=int32)&gt;\n\n\n- (1,2) concat (1,2) =&gt; (1,4) - 첫번째 // axis=0\n- (2,3,4,5) concat (2,3,4,5) =&gt; (4,3,4,5) - 첫번째 // axis=0\n\na=tf.reshape(tf.constant(range(120)),(2,3,4,5))\nb=-a\n\n\ntf.concat([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(4, 3, 4, 5), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4],\n         [   5,    6,    7,    8,    9],\n         [  10,   11,   12,   13,   14],\n         [  15,   16,   17,   18,   19]],\n\n        [[  20,   21,   22,   23,   24],\n         [  25,   26,   27,   28,   29],\n         [  30,   31,   32,   33,   34],\n         [  35,   36,   37,   38,   39]],\n\n        [[  40,   41,   42,   43,   44],\n         [  45,   46,   47,   48,   49],\n         [  50,   51,   52,   53,   54],\n         [  55,   56,   57,   58,   59]]],\n\n\n       [[[  60,   61,   62,   63,   64],\n         [  65,   66,   67,   68,   69],\n         [  70,   71,   72,   73,   74],\n         [  75,   76,   77,   78,   79]],\n\n        [[  80,   81,   82,   83,   84],\n         [  85,   86,   87,   88,   89],\n         [  90,   91,   92,   93,   94],\n         [  95,   96,   97,   98,   99]],\n\n        [[ 100,  101,  102,  103,  104],\n         [ 105,  106,  107,  108,  109],\n         [ 110,  111,  112,  113,  114],\n         [ 115,  116,  117,  118,  119]]],\n\n\n       [[[   0,   -1,   -2,   -3,   -4],\n         [  -5,   -6,   -7,   -8,   -9],\n         [ -10,  -11,  -12,  -13,  -14],\n         [ -15,  -16,  -17,  -18,  -19]],\n\n        [[ -20,  -21,  -22,  -23,  -24],\n         [ -25,  -26,  -27,  -28,  -29],\n         [ -30,  -31,  -32,  -33,  -34],\n         [ -35,  -36,  -37,  -38,  -39]],\n\n        [[ -40,  -41,  -42,  -43,  -44],\n         [ -45,  -46,  -47,  -48,  -49],\n         [ -50,  -51,  -52,  -53,  -54],\n         [ -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[ -60,  -61,  -62,  -63,  -64],\n         [ -65,  -66,  -67,  -68,  -69],\n         [ -70,  -71,  -72,  -73,  -74],\n         [ -75,  -76,  -77,  -78,  -79]],\n\n        [[ -80,  -81,  -82,  -83,  -84],\n         [ -85,  -86,  -87,  -88,  -89],\n         [ -90,  -91,  -92,  -93,  -94],\n         [ -95,  -96,  -97,  -98,  -99]],\n\n        [[-100, -101, -102, -103, -104],\n         [-105, -106, -107, -108, -109],\n         [-110, -111, -112, -113, -114],\n         [-115, -116, -117, -118, -119]]]], dtype=int32)&gt;\n\n\n- (2,3,4,5) concat (2,3,4,5) =&gt; (2,6,4,5) - 두번째 // axis=1\n\na=tf.reshape(tf.constant(range(120)),(2,3,4,5))\nb=-a\n\n\ntf.concat([a,b],axis=1)\n\n&lt;tf.Tensor: shape=(2, 6, 4, 5), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4],\n         [   5,    6,    7,    8,    9],\n         [  10,   11,   12,   13,   14],\n         [  15,   16,   17,   18,   19]],\n\n        [[  20,   21,   22,   23,   24],\n         [  25,   26,   27,   28,   29],\n         [  30,   31,   32,   33,   34],\n         [  35,   36,   37,   38,   39]],\n\n        [[  40,   41,   42,   43,   44],\n         [  45,   46,   47,   48,   49],\n         [  50,   51,   52,   53,   54],\n         [  55,   56,   57,   58,   59]],\n\n        [[   0,   -1,   -2,   -3,   -4],\n         [  -5,   -6,   -7,   -8,   -9],\n         [ -10,  -11,  -12,  -13,  -14],\n         [ -15,  -16,  -17,  -18,  -19]],\n\n        [[ -20,  -21,  -22,  -23,  -24],\n         [ -25,  -26,  -27,  -28,  -29],\n         [ -30,  -31,  -32,  -33,  -34],\n         [ -35,  -36,  -37,  -38,  -39]],\n\n        [[ -40,  -41,  -42,  -43,  -44],\n         [ -45,  -46,  -47,  -48,  -49],\n         [ -50,  -51,  -52,  -53,  -54],\n         [ -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[  60,   61,   62,   63,   64],\n         [  65,   66,   67,   68,   69],\n         [  70,   71,   72,   73,   74],\n         [  75,   76,   77,   78,   79]],\n\n        [[  80,   81,   82,   83,   84],\n         [  85,   86,   87,   88,   89],\n         [  90,   91,   92,   93,   94],\n         [  95,   96,   97,   98,   99]],\n\n        [[ 100,  101,  102,  103,  104],\n         [ 105,  106,  107,  108,  109],\n         [ 110,  111,  112,  113,  114],\n         [ 115,  116,  117,  118,  119]],\n\n        [[ -60,  -61,  -62,  -63,  -64],\n         [ -65,  -66,  -67,  -68,  -69],\n         [ -70,  -71,  -72,  -73,  -74],\n         [ -75,  -76,  -77,  -78,  -79]],\n\n        [[ -80,  -81,  -82,  -83,  -84],\n         [ -85,  -86,  -87,  -88,  -89],\n         [ -90,  -91,  -92,  -93,  -94],\n         [ -95,  -96,  -97,  -98,  -99]],\n\n        [[-100, -101, -102, -103, -104],\n         [-105, -106, -107, -108, -109],\n         [-110, -111, -112, -113, -114],\n         [-115, -116, -117, -118, -119]]]], dtype=int32)&gt;\n\n\n- (2,3,4,5) concat (2,3,4,5) =&gt; (2,3,8,5) - 세번째 // axis=2\n\na=tf.reshape(tf.constant(range(120)),(2,3,4,5))\nb=-a\n\n\ntf.concat([a,b],axis=2)\n\n&lt;tf.Tensor: shape=(2, 3, 8, 5), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4],\n         [   5,    6,    7,    8,    9],\n         [  10,   11,   12,   13,   14],\n         [  15,   16,   17,   18,   19],\n         [   0,   -1,   -2,   -3,   -4],\n         [  -5,   -6,   -7,   -8,   -9],\n         [ -10,  -11,  -12,  -13,  -14],\n         [ -15,  -16,  -17,  -18,  -19]],\n\n        [[  20,   21,   22,   23,   24],\n         [  25,   26,   27,   28,   29],\n         [  30,   31,   32,   33,   34],\n         [  35,   36,   37,   38,   39],\n         [ -20,  -21,  -22,  -23,  -24],\n         [ -25,  -26,  -27,  -28,  -29],\n         [ -30,  -31,  -32,  -33,  -34],\n         [ -35,  -36,  -37,  -38,  -39]],\n\n        [[  40,   41,   42,   43,   44],\n         [  45,   46,   47,   48,   49],\n         [  50,   51,   52,   53,   54],\n         [  55,   56,   57,   58,   59],\n         [ -40,  -41,  -42,  -43,  -44],\n         [ -45,  -46,  -47,  -48,  -49],\n         [ -50,  -51,  -52,  -53,  -54],\n         [ -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[  60,   61,   62,   63,   64],\n         [  65,   66,   67,   68,   69],\n         [  70,   71,   72,   73,   74],\n         [  75,   76,   77,   78,   79],\n         [ -60,  -61,  -62,  -63,  -64],\n         [ -65,  -66,  -67,  -68,  -69],\n         [ -70,  -71,  -72,  -73,  -74],\n         [ -75,  -76,  -77,  -78,  -79]],\n\n        [[  80,   81,   82,   83,   84],\n         [  85,   86,   87,   88,   89],\n         [  90,   91,   92,   93,   94],\n         [  95,   96,   97,   98,   99],\n         [ -80,  -81,  -82,  -83,  -84],\n         [ -85,  -86,  -87,  -88,  -89],\n         [ -90,  -91,  -92,  -93,  -94],\n         [ -95,  -96,  -97,  -98,  -99]],\n\n        [[ 100,  101,  102,  103,  104],\n         [ 105,  106,  107,  108,  109],\n         [ 110,  111,  112,  113,  114],\n         [ 115,  116,  117,  118,  119],\n         [-100, -101, -102, -103, -104],\n         [-105, -106, -107, -108, -109],\n         [-110, -111, -112, -113, -114],\n         [-115, -116, -117, -118, -119]]]], dtype=int32)&gt;\n\n\n- (2,3,4,5) concat (2,3,4,5) =&gt; (2,3,4,10) - 네번째 // axis=3 # 0,1,2,3 // -4 -3 -2 -1\n\na=tf.reshape(tf.constant(range(120)),(2,3,4,5))\nb=-a\n\n\ntf.concat([a,b],axis=-1)\n\n&lt;tf.Tensor: shape=(2, 3, 4, 10), dtype=int32, numpy=\narray([[[[   0,    1,    2,    3,    4,    0,   -1,   -2,   -3,   -4],\n         [   5,    6,    7,    8,    9,   -5,   -6,   -7,   -8,   -9],\n         [  10,   11,   12,   13,   14,  -10,  -11,  -12,  -13,  -14],\n         [  15,   16,   17,   18,   19,  -15,  -16,  -17,  -18,  -19]],\n\n        [[  20,   21,   22,   23,   24,  -20,  -21,  -22,  -23,  -24],\n         [  25,   26,   27,   28,   29,  -25,  -26,  -27,  -28,  -29],\n         [  30,   31,   32,   33,   34,  -30,  -31,  -32,  -33,  -34],\n         [  35,   36,   37,   38,   39,  -35,  -36,  -37,  -38,  -39]],\n\n        [[  40,   41,   42,   43,   44,  -40,  -41,  -42,  -43,  -44],\n         [  45,   46,   47,   48,   49,  -45,  -46,  -47,  -48,  -49],\n         [  50,   51,   52,   53,   54,  -50,  -51,  -52,  -53,  -54],\n         [  55,   56,   57,   58,   59,  -55,  -56,  -57,  -58,  -59]]],\n\n\n       [[[  60,   61,   62,   63,   64,  -60,  -61,  -62,  -63,  -64],\n         [  65,   66,   67,   68,   69,  -65,  -66,  -67,  -68,  -69],\n         [  70,   71,   72,   73,   74,  -70,  -71,  -72,  -73,  -74],\n         [  75,   76,   77,   78,   79,  -75,  -76,  -77,  -78,  -79]],\n\n        [[  80,   81,   82,   83,   84,  -80,  -81,  -82,  -83,  -84],\n         [  85,   86,   87,   88,   89,  -85,  -86,  -87,  -88,  -89],\n         [  90,   91,   92,   93,   94,  -90,  -91,  -92,  -93,  -94],\n         [  95,   96,   97,   98,   99,  -95,  -96,  -97,  -98,  -99]],\n\n        [[ 100,  101,  102,  103,  104, -100, -101, -102, -103, -104],\n         [ 105,  106,  107,  108,  109, -105, -106, -107, -108, -109],\n         [ 110,  111,  112,  113,  114, -110, -111, -112, -113, -114],\n         [ 115,  116,  117,  118,  119, -115, -116, -117, -118, -119]]]],\n      dtype=int32)&gt;\n\n\n- (4,) concat (4,) =&gt; (8,) - 첫번째축? // axis=0\n\na=tf.constant([1,2,3,4])\nb=-a \na,b\n\n(&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;,\n &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;)\n\n\n\ntf.concat([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(8,), dtype=int32, numpy=array([ 1,  2,  3,  4, -1, -2, -3, -4], dtype=int32)&gt;\n\n\n- (4,) concat (4,) =&gt; (4,2) - 두번째축? // axis=1 ==&gt; 이런거없다..\n\na=tf.constant([1,2,3,4])\nb=-a \na,b\n\n(&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;,\n &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;)\n\n\n\ntf.concat([a,b],axis=1)\n\nInvalidArgumentError: ConcatOp : Expected concatenating dimensions in the range [-1, 1), but got 1 [Op:ConcatV2] name: concat\n\n\n\n\ntf.stack\n\n-&gt; Stacks a list of rank-R tensors into one rank-(R+1) tensor\n\na=tf.constant([1,2,3,4])\nb=-a \na,b\n\n(&lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 2, 3, 4], dtype=int32)&gt;,\n &lt;tf.Tensor: shape=(4,), dtype=int32, numpy=array([-1, -2, -3, -4], dtype=int32)&gt;)\n\n\n\ntf.stack([a,b],axis=0)\n\n&lt;tf.Tensor: shape=(2, 4), dtype=int32, numpy=\narray([[ 1,  2,  3,  4],\n       [-1, -2, -3, -4]], dtype=int32)&gt;\n\n\n\ntf.stack([a,b],axis=1)\n\n&lt;tf.Tensor: shape=(4, 2), dtype=int32, numpy=\narray([[ 1, -1],\n       [ 2, -2],\n       [ 3, -3],\n       [ 4, -4]], dtype=int32)&gt;\n\n\n\n\n\n\ntnp\n- tf는 넘파이에 비하여 텐서만들기가 너무힘듬\n\nnp.diag([1,2,3]).reshape(-1)\n\narray([1, 0, 0, 0, 2, 0, 0, 0, 3])\n\n\n\n넘파이는 이런식으로 np.diag()도 쓸수 있고 reshape을 메소드로 쓸 수도 있는데…\n\n\ntnp 사용방법 (불만해결방법)\n\nimport tensorflow.experimental.numpy as tnp\ntnp.experimental_enable_numpy_behavior()\n\n\ntype(tnp.array([1,2,3]))\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n- int와 float을 더할 수 있음\n\ntnp.array([1,2,3])+tnp.array([1.0,2.0,3.0])\n\n&lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 6.])&gt;\n\n\n\ntf.constant([1,2,3])+tf.constant([1.0,2.0,3.0])\n\n&lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 4., 6.])&gt;\n\n\n\ntnp.array(1)+tnp.array([1.0,2.0,3.0])\n\n&lt;tf.Tensor: shape=(3,), dtype=float64, numpy=array([2., 3., 4.])&gt;\n\n\n\ntnp.diag([1,2,3])\n\n&lt;tf.Tensor: shape=(3, 3), dtype=int64, numpy=\narray([[1, 0, 0],\n       [0, 2, 0],\n       [0, 0, 3]])&gt;\n\n\n\na=tnp.diag([1,2,3])\ntype(a)\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\na=tf.constant([1,2,3])\na.reshape(3,1)\n\n&lt;tf.Tensor: shape=(3, 1), dtype=int32, numpy=\narray([[1],\n       [2],\n       [3]], dtype=int32)&gt;\n\n\n\n\n선언고급\n\nnp.random.randn(5)\n\narray([0.67533519, 0.18494521, 0.76946432, 0.94461951, 1.15058192])\n\n\n\ntnp.random.randn(5) # 넘파이가 되면 나도 된다.\n\n&lt;tf.Tensor: shape=(5,), dtype=float64, numpy=array([-0.37112581, -0.31535817, -0.92963552, -0.68741888, -0.54859424])&gt;\n\n\n\n\n타입\n\ntype(tnp.random.randn(5))\n\ntensorflow.python.framework.ops.EagerTensor\n\n\n\n\ntf.contant로 만들어도 마치 넘파이인듯 쓰는 기능들\n- 묵시적형변환이 가능???\n\ntf.constant([1,1])+tf.constant([2.2,3.3]) # 브로드캐스팅 기능 : 모양이 다른 텐서들 간에 산술연산을 수행할 떄 TensorFlow가 텐서의 모양을 자동으로 맞춰줌\n\n&lt;tf.Tensor: shape=(2,), dtype=float64, numpy=array([3.20000005, 4.29999995])&gt;\n\n\n- 메소드를 쓸수 있음.\n\na= tnp.array([[1,2,3,4]])\na.T\n\n&lt;tf.Tensor: shape=(4, 1), dtype=int64, numpy=\narray([[1],\n       [2],\n       [3],\n       [4]])&gt;\n\n\n\n\n그렇지만 np.array는 아님\n- 원소를 할당하는것은 불가능\n\na=tf.constant([1,2,3])\na\n\n&lt;tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 2, 3], dtype=int32)&gt;\n\n\n\na[0]=11\n\nTypeError: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment\n\n\n\n\n\n그 밖의 : TensorFlow 활용 가이드\nhttps://www.tensorflow.org/api_docs/python/tf/Tensor"
  },
  {
    "objectID": "posts/study/2023-08-28-Extra 00. tidydata.html",
    "href": "posts/study/2023-08-28-Extra 00. tidydata.html",
    "title": "Extra 00. tidydata",
    "section": "",
    "text": "- 내가 생각하는 데이터 분석의 단계\n\n데이터의 기본 정보 (df.info 등등)을 확인\n데이터를 tidy하게 바꾼 후 EDA(탐색적 자료 분석), 통계 분석(t-test) 등등 수행\ntidy데이터를 바탕으로 모델링 수행\n결과 요약 및 해석 -&gt; 결과에 대한 자료 작성 시에도 tidy한 데이터가 사용됨\n\n- 금일 발표 요약\n\ntidy 데이터의 정의\ntidy 데이터를 활용하지 않은 시각화 (matplitlib)\ntidy 데이터를 활용한 시각화 (seaborn, ggplot, plotly 등등…)\n심슨의 역설"
  },
  {
    "objectID": "posts/study/2023-08-28-Extra 00. tidydata.html#데이터-생성-tidy-x",
    "href": "posts/study/2023-08-28-Extra 00. tidydata.html#데이터-생성-tidy-x",
    "title": "Extra 00. tidydata",
    "section": "데이터 생성 (tidy : (x))",
    "text": "데이터 생성 (tidy : (x))\n\n\nCode\nmale_w = np.random.randint(60,100,10000) + np.random.randn(10000)\nfemale_w = np.random.randint(40,80,10000) + np.random.randn(10000)\nmale_h = (np.random.randint(160,200,10000) + np.random.randn(10000))/100\nfemale_h = (np.random.randint(140,180,10000) + np.random.randn(10000))/100\n\n\nbmi_m =  male_w/(male_h**2)\nbmi_f =  female_w/(female_h**2)\n\nbins = [-np.Inf, 18.5,23,25,np.Inf]\nlabels = [\"저체중\",\"정상\",\"과체중\",\"비만\"]\n\ndf = pd.DataFrame([male_w,female_w,male_h,female_h,bmi_m,bmi_f]).T\n\ndf.columns = [\"male_w\",\"female_w\",\"male_h\",\"female_h\",\"bmi_m\",\"bmi_f\"]\n\n\ndf[\"bmi_label_M\"] = pd.cut(df.bmi_m, bins = bins, labels =labels)\ndf[\"bmi_label_F\"] = pd.cut(df.bmi_f, bins = bins, labels =labels)\n\n\nbins_m = [-np.Inf, 1.60,1.70,1.80,1.90,np.Inf]\nlabels = [\"매우 작은키\",\"작은키\",\"보통\",\"큰키\",\"매우 큰키\"]\n\nbins_f = [-np.Inf, 1.40,1.50,1.60,1.70,np.Inf]\n\n\ndf[\"h_label_M\"] = pd.cut(df.male_h, bins = bins_m, labels =labels)\ndf[\"h_label_F\"] = pd.cut(df.female_h, bins = bins_f, labels =labels)\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\nmale_w\nfemale_w\nmale_h\nfemale_h\nbmi_m\nbmi_f\nbmi_label_M\nbmi_label_F\nh_label_M\nh_label_F\n\n\n\n\n0\n95.512228\n77.760589\n1.747844\n1.555205\n31.264669\n32.150229\n비만\n비만\n보통\n보통\n\n\n1\n78.657688\n65.837765\n1.648885\n1.572286\n28.930805\n26.632491\n비만\n비만\n작은키\n보통\n\n\n2\n92.135422\n41.169597\n1.756829\n1.611968\n29.851614\n15.843971\n비만\n저체중\n보통\n큰키\n\n\n3\n85.688058\n55.415655\n1.750162\n1.560285\n27.974579\n22.762752\n비만\n정상\n보통\n보통\n\n\n4\n83.454618\n43.582328\n1.625592\n1.502553\n31.581097\n19.304156\n비만\n정상\n작은키\n보통"
  },
  {
    "objectID": "posts/study/2023-08-28-Extra 00. tidydata.html#시각화-1-matplotlib-tidy-x",
    "href": "posts/study/2023-08-28-Extra 00. tidydata.html#시각화-1-matplotlib-tidy-x",
    "title": "Extra 00. tidydata",
    "section": "시각화 1 (matplotlib, tidy x)",
    "text": "시각화 1 (matplotlib, tidy x)\n- 일단 남자와 여자의 BMI 지수를 비교하고 싶음\n- 데이터 구조\n\ndf.head()\n\n\n\n\n\n\n\n\nmale_w\nfemale_w\nmale_h\nfemale_h\nbmi_m\nbmi_f\nbmi_label_M\nbmi_label_F\nh_label_M\nh_label_F\n\n\n\n\n0\n69.401280\n59.315255\n1.625395\n1.588538\n26.269360\n23.505598\n비만\n과체중\n작은키\n보통\n\n\n1\n96.582571\n71.060495\n1.754996\n1.704732\n31.357869\n24.452105\n비만\n과체중\n보통\n매우 큰키\n\n\n2\n64.515219\n73.489190\n1.891025\n1.616301\n18.041283\n28.130585\n저체중\n비만\n큰키\n큰키\n\n\n3\n96.048789\n56.877694\n1.831055\n1.723334\n28.647643\n19.151515\n비만\n정상\n큰키\n매우 큰키\n\n\n4\n91.834611\n69.049447\n1.726567\n1.726978\n30.806303\n23.151891\n비만\n과체중\n보통\n매우 큰키\n\n\n\n\n\n\n\n- 시각화\n\nplt.figure(figsize=(12,4))\nplt.plot(df.male_w,df.bmi_m,\".\",alpha=0.3)\nplt.plot(df.female_w,df.bmi_f,\".\",alpha=0.3)\nplt.legend([\"남\",\"여\"])\nplt.title(\"남자와 여자 몸무게에 따른 BMI\")\n\nText(0.5, 1.0, '남자와 여자 몸무게에 따른 BMI')"
  },
  {
    "objectID": "posts/study/2023-08-28-Extra 00. tidydata.html#시각화-2-matplotlib.pyplot-tidy-x-subplot",
    "href": "posts/study/2023-08-28-Extra 00. tidydata.html#시각화-2-matplotlib.pyplot-tidy-x-subplot",
    "title": "Extra 00. tidydata",
    "section": "시각화 2 (matplotlib.pyplot, tidy x, subplot)",
    "text": "시각화 2 (matplotlib.pyplot, tidy x, subplot)\n\nfig, axes = plt.subplots(1,2,figsize=(12,4))\n\nax1,ax2 = axes\n\nax1.plot(df.male_w,df.bmi_m,\".b\",alpha=0.3)\nax1.set_title(\"남자 몸무게에 따른 BMI\")\nax2.plot(df.female_w,df.bmi_f,\".r\",alpha=0.3)\nax2.set_title(\"여자 몸무게에 따른 BMI\")\n\nText(0.5, 1.0, '여자 몸무게에 따른 BMI')"
  },
  {
    "objectID": "posts/study/2023-08-28-Extra 00. tidydata.html#의문-1",
    "href": "posts/study/2023-08-28-Extra 00. tidydata.html#의문-1",
    "title": "Extra 00. tidydata",
    "section": "의문 1",
    "text": "의문 1\n- 아래의 데이터 구조를 살펴보자.\n\ndf.head()\n\n\n\n\n\n\n\n\nmale_w\nfemale_w\nmale_h\nfemale_h\nbmi_m\nbmi_f\nbmi_label_M\nbmi_label_F\nh_label_M\nh_label_F\n\n\n\n\n0\n72.570505\n53.415130\n1.773998\n1.671568\n23.059717\n19.116853\n과체중\n정상\n보통\n큰키\n\n\n1\n93.896500\n43.278004\n1.883459\n1.612035\n26.468980\n16.653995\n비만\n저체중\n큰키\n큰키\n\n\n2\n82.263387\n71.909528\n1.607014\n1.524727\n31.854236\n30.931608\n비만\n비만\n작은키\n보통\n\n\n3\n99.802621\n64.476794\n1.871943\n1.761914\n28.481097\n20.769874\n비만\n정상\n큰키\n매우 큰키\n\n\n4\n78.356406\n42.978916\n1.658058\n1.694691\n28.501984\n14.964927\n비만\n저체중\n작은키\n큰키\n\n\n\n\n\n\n\n- 분석가들에게 시각화 및 모델링 시 편한것\n\n한 컬럼에 모든 데이터의 대한 특정 수치(몸무게면 몸무게, 키면 키)가 있으면 좋겠음\n그리고 또 다른 컬럼에는 그것의 범주(성별, BmI수준)을 알려주는 컬럼이 있었으면 좋겠음\n이게 편한 이유? 시각화 수행 시 아래의 긴 코드를 한 줄로 쓸 수 있다!! \\(\\to\\) (지금은 남,녀 두 개의 범주지만, 범주의 수가 늘어나면 코드 라인 수도 늘어난다.)\n\nplt.figure(figsize=(12,4))\nplt.plot(df.male_w,df.bmi_m,\".\",alpha=0.3)\nplt.plot(df.female_w,df.bmi_f,\".\",alpha=0.3)\nplt.legend([\"남\",\"여\"])\nplt.title(\"남자와 여자 몸무게에 따른 BMI\")\n\n또한, 모델링 수행시 입력데이터는 tidy한 형태로 들어가기 때문이다.(이미지, 텍스트 제외)"
  },
  {
    "objectID": "posts/study/2023-08-28-Extra 00. tidydata.html#tidydata-만들기",
    "href": "posts/study/2023-08-28-Extra 00. tidydata.html#tidydata-만들기",
    "title": "Extra 00. tidydata",
    "section": "tidydata 만들기",
    "text": "tidydata 만들기"
  },
  {
    "objectID": "posts/study/2023-08-28-Extra 00. tidydata.html#인데스를-id로-변경",
    "href": "posts/study/2023-08-28-Extra 00. tidydata.html#인데스를-id로-변경",
    "title": "Extra 00. tidydata",
    "section": "1. 인데스를 ID로 변경",
    "text": "1. 인데스를 ID로 변경\n\ndf.head()\n\n\n\n\n\n\n\n\nmale_w\nfemale_w\nmale_h\nfemale_h\nbmi_m\nbmi_f\nbmi_label_M\nbmi_label_F\nh_label_M\nh_label_F\n\n\n\n\n0\n95.512228\n77.760589\n1.747844\n1.555205\n31.264669\n32.150229\n비만\n비만\n보통\n보통\n\n\n1\n78.657688\n65.837765\n1.648885\n1.572286\n28.930805\n26.632491\n비만\n비만\n작은키\n보통\n\n\n2\n92.135422\n41.169597\n1.756829\n1.611968\n29.851614\n15.843971\n비만\n저체중\n보통\n큰키\n\n\n3\n85.688058\n55.415655\n1.750162\n1.560285\n27.974579\n22.762752\n비만\n정상\n보통\n보통\n\n\n4\n83.454618\n43.582328\n1.625592\n1.502553\n31.581097\n19.304156\n비만\n정상\n작은키\n보통\n\n\n\n\n\n\n\n\ndf.reset_index(inplace=True)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nindex\nmale_w\nfemale_w\nmale_h\nfemale_h\nbmi_m\nbmi_f\nbmi_label_M\nbmi_label_F\nh_label_M\nh_label_F\n\n\n\n\n0\n0\n95.512228\n77.760589\n1.747844\n1.555205\n31.264669\n32.150229\n비만\n비만\n보통\n보통\n\n\n1\n1\n78.657688\n65.837765\n1.648885\n1.572286\n28.930805\n26.632491\n비만\n비만\n작은키\n보통\n\n\n2\n2\n92.135422\n41.169597\n1.756829\n1.611968\n29.851614\n15.843971\n비만\n저체중\n보통\n큰키\n\n\n3\n3\n85.688058\n55.415655\n1.750162\n1.560285\n27.974579\n22.762752\n비만\n정상\n보통\n보통\n\n\n4\n4\n83.454618\n43.582328\n1.625592\n1.502553\n31.581097\n19.304156\n비만\n정상\n작은키\n보통\n\n\n\n\n\n\n\n\ndf.rename(columns={\"index\" : \"ID\" },inplace=True)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nID\nmale_w\nfemale_w\nmale_h\nfemale_h\nbmi_m\nbmi_f\nbmi_label_M\nbmi_label_F\nh_label_M\nh_label_F\n\n\n\n\n0\n0\n95.512228\n77.760589\n1.747844\n1.555205\n31.264669\n32.150229\n비만\n비만\n보통\n보통\n\n\n1\n1\n78.657688\n65.837765\n1.648885\n1.572286\n28.930805\n26.632491\n비만\n비만\n작은키\n보통\n\n\n2\n2\n92.135422\n41.169597\n1.756829\n1.611968\n29.851614\n15.843971\n비만\n저체중\n보통\n큰키\n\n\n3\n3\n85.688058\n55.415655\n1.750162\n1.560285\n27.974579\n22.762752\n비만\n정상\n보통\n보통\n\n\n4\n4\n83.454618\n43.582328\n1.625592\n1.502553\n31.581097\n19.304156\n비만\n정상\n작은키\n보통"
  },
  {
    "objectID": "posts/study/2023-08-28-Extra 00. tidydata.html#df.meltweight-테이블-생성",
    "href": "posts/study/2023-08-28-Extra 00. tidydata.html#df.meltweight-테이블-생성",
    "title": "Extra 00. tidydata",
    "section": "2. df.melt(weight) 테이블 생성",
    "text": "2. df.melt(weight) 테이블 생성\n- 한 열에는 셩별(sex), 다른 한 열에는 무게(weight)를 표시\n\ndf.loc[:,[\"ID\",\"male_w\",\"female_w\"]].head(4)\n\n\n\n\n\n\n\n\nID\nmale_w\nfemale_w\n\n\n\n\n0\n0\n95.512228\n77.760589\n\n\n1\n1\n78.657688\n65.837765\n\n\n2\n2\n92.135422\n41.169597\n\n\n3\n3\n85.688058\n55.415655\n\n\n\n\n\n\n\n\ndf.loc[:,[\"ID\",\"male_w\",\"female_w\"]].\\\n        melt(id_vars=[\"ID\"],\n        value_vars = [\"male_w\",\"female_w\"],\n        var_name = \"sex\",value_name =\"weight\")\n\n\n\n\n\n\n\n\nID\nsex\nweight\n\n\n\n\n0\n0\nmale_w\n95.512228\n\n\n1\n1\nmale_w\n78.657688\n\n\n2\n2\nmale_w\n92.135422\n\n\n3\n3\nmale_w\n85.688058\n\n\n4\n4\nmale_w\n83.454618\n\n\n...\n...\n...\n...\n\n\n19995\n9995\nfemale_w\n72.715375\n\n\n19996\n9996\nfemale_w\n78.467711\n\n\n19997\n9997\nfemale_w\n67.947894\n\n\n19998\n9998\nfemale_w\n51.354418\n\n\n19999\n9999\nfemale_w\n48.809833\n\n\n\n\n20000 rows × 3 columns\n\n\n\n\nweight = df.melt(id_vars=[\"ID\"],\n            value_vars=[\"male_w\",\"female_w\"],\n            var_name = \"sex\",value_name='weight')\n\nweight.sex = weight.sex.replace(\"male_w\",\"m\").replace(\"female_w\",\"f\")\nweight.head()\n\n\n\n\n\n\n\n\nID\nsex\nweight\n\n\n\n\n0\n0\nm\n69.401280\n\n\n1\n1\nm\n96.582571\n\n\n2\n2\nm\n64.515219\n\n\n3\n3\nm\n96.048789\n\n\n4\n4\nm\n91.834611\n\n\n\n\n\n\n\n\n생성한 테이블을 다시 돌릴려면?\n\n_t = weight.pivot(index=\"ID\",columns = \"sex\", values = \"weight\").head()\n_t.columns = [\"male_w\",\"male_f\"]\n_t.reset_index()\n\n\n\n\n\n\n\n\nID\nmale_w\nmale_f\n\n\n\n\n0\n0\n59.315255\n69.401280\n\n\n1\n1\n71.060495\n96.582571\n\n\n2\n2\n73.489190\n64.515219\n\n\n3\n3\n56.877694\n96.048789\n\n\n4\n4\n69.049447\n91.834611"
  },
  {
    "objectID": "posts/study/2023-08-28-Extra 00. tidydata.html#df.meltbmi-테이블-생성",
    "href": "posts/study/2023-08-28-Extra 00. tidydata.html#df.meltbmi-테이블-생성",
    "title": "Extra 00. tidydata",
    "section": "3. df.melt(BMI) 테이블 생성",
    "text": "3. df.melt(BMI) 테이블 생성\n\nbmi = df.melt(id_vars=[\"ID\"],\n            value_vars=[\"bmi_f\",\"bmi_m\"],\n            var_name = \"sex\",value_name='bmi')\n\nbmi.sex = bmi.sex.replace(\"bmi_m\",\"m\").replace(\"bmi_f\",\"f\")\nbmi.head()\n\n\n\n\n\n\n\n\nID\nsex\nbmi\n\n\n\n\n0\n0\nf\n23.505598\n\n\n1\n1\nf\n24.452105\n\n\n2\n2\nf\n28.130585\n\n\n3\n3\nf\n19.151515\n\n\n4\n4\nf\n23.151891"
  },
  {
    "objectID": "posts/study/2023-08-28-Extra 00. tidydata.html#weight-테이블과-bmi-테이블-조인",
    "href": "posts/study/2023-08-28-Extra 00. tidydata.html#weight-테이블과-bmi-테이블-조인",
    "title": "Extra 00. tidydata",
    "section": "4. weight 테이블과 bmi 테이블 조인",
    "text": "4. weight 테이블과 bmi 테이블 조인\n\ndf1 = pd.merge(weight,bmi, \n         left_on=['ID','sex'], right_on=['ID','sex'])\n\n\ndf1.head()\n\n\n\n\n\n\n\n\nID\nsex\nweight\nbmi\n\n\n\n\n0\n0\nm\n69.401280\n26.269360\n\n\n1\n1\nm\n96.582571\n31.357869\n\n\n2\n2\nm\n64.515219\n18.041283\n\n\n3\n3\nm\n96.048789\n28.647643\n\n\n4\n4\nm\n91.834611\n30.806303"
  },
  {
    "objectID": "posts/study/2023-08-28-Extra 00. tidydata.html#시각화-3plotlytidy-o",
    "href": "posts/study/2023-08-28-Extra 00. tidydata.html#시각화-3plotlytidy-o",
    "title": "Extra 00. tidydata",
    "section": "5. 시각화 3(plotly,tidy (o))",
    "text": "5. 시각화 3(plotly,tidy (o))\n\ndf1.plot(x=\"weight\", y= \"bmi\",backend=\"plotly\",kind=\"scatter\",\n                color= \"sex\", title= \"남녀 몸무게에 따른 BMI\")"
  },
  {
    "objectID": "posts/study/2023-08-28-Extra 00. tidydata.html#시각화-4plotlytidy-o",
    "href": "posts/study/2023-08-28-Extra 00. tidydata.html#시각화-4plotlytidy-o",
    "title": "Extra 00. tidydata",
    "section": "6. 시각화 4(plotly,tidy (o))",
    "text": "6. 시각화 4(plotly,tidy (o))\n\ndf1.plot(x=\"weight\", y= \"bmi\",backend=\"plotly\",kind=\"scatter\",\n                color= \"sex\", title= \"남녀 몸무게에 따른 BMI\",facet_col=df1.sex)"
  },
  {
    "objectID": "posts/study/2023-08-28-Extra 00. tidydata.html#데이터-비교",
    "href": "posts/study/2023-08-28-Extra 00. tidydata.html#데이터-비교",
    "title": "Extra 00. tidydata",
    "section": "7. 데이터 비교",
    "text": "7. 데이터 비교\n1 tidy (x)\n\ndf.iloc[:,[0,1,2,5,6]].head()\n\n\n\n\n\n\n\n\nID\nmale_w\nfemale_w\nbmi_m\nbmi_f\n\n\n\n\n0\n0\n69.401280\n59.315255\n26.269360\n23.505598\n\n\n1\n1\n96.582571\n71.060495\n31.357869\n24.452105\n\n\n2\n2\n64.515219\n73.489190\n18.041283\n28.130585\n\n\n3\n3\n96.048789\n56.877694\n28.647643\n19.151515\n\n\n4\n4\n91.834611\n69.049447\n30.806303\n23.151891\n\n\n\n\n\n\n\n2 tidy (o)\n\ndf1.head()\n\n\n\n\n\n\n\n\nID\nsex\nweight\nbmi\n\n\n\n\n0\n0\nm\n69.401280\n26.269360\n\n\n1\n1\nm\n96.582571\n31.357869\n\n\n2\n2\nm\n64.515219\n18.041283\n\n\n3\n3\nm\n96.048789\n28.647643\n\n\n4\n4\nm\n91.834611\n30.806303"
  },
  {
    "objectID": "posts/study/2023-08-28-Extra 00. tidydata.html#코드-비교",
    "href": "posts/study/2023-08-28-Extra 00. tidydata.html#코드-비교",
    "title": "Extra 00. tidydata",
    "section": "8. 코드 비교",
    "text": "8. 코드 비교\n1 tidy(x)\nplt.figure(figsize=(12,4))\nplt.plot(df.male_w,df.bmi_m,\".\",alpha=0.3)\nplt.plot(df.female_w,df.bmi_f,\".\",alpha=0.3)\nplt.legend([\"남\",\"여\"])\nplt.title(\"남자와 여자 몸무게에 따른 BMI\")\nfig, axes = plt.subplots(1,2,figsize=(12,4))\nax1,ax2 = axes\nax1.plot(df.male_w,df.bmi_m,\".b\",alpha=0.3)\nax1.set_title(\"남자 몸무게에 따른 BMI\")\nax2.plot(df.female_w,df.bmi_f,\".r\",alpha=0.3)\nax2.set_title(\"여자 몸무게에 따른 BMI\")\n2 tidy (o)\ndf1.plot(x=\"weight\", y= \"bmi\",backend=\"plotly\",kind=\"scatter\",\n                color= \"sex\", title= \"남녀 몸무게에 따른 BMI\")\ndf1.plot(x=\"weight\", y= \"bmi\",backend=\"plotly\",kind=\"scatter\",\n                color= \"sex\", title= \"남녀 몸무게에 따른 BMI\",facet_col=df1.sex)"
  },
  {
    "objectID": "posts/study/2023-08-28-Extra 00. tidydata.html#다차원-plot-height-label",
    "href": "posts/study/2023-08-28-Extra 00. tidydata.html#다차원-plot-height-label",
    "title": "Extra 00. tidydata",
    "section": "9. 다차원 plot (+height label)",
    "text": "9. 다차원 plot (+height label)\n\ndf.head()\n\n\n\n\n\n\n\n\nID\nmale_w\nfemale_w\nmale_h\nfemale_h\nbmi_m\nbmi_f\nbmi_label_M\nbmi_label_F\nh_label_M\nh_label_F\n\n\n\n\n0\n0\n69.401280\n59.315255\n1.625395\n1.588538\n26.269360\n23.505598\n비만\n과체중\n작은키\n보통\n\n\n1\n1\n96.582571\n71.060495\n1.754996\n1.704732\n31.357869\n24.452105\n비만\n과체중\n보통\n매우 큰키\n\n\n2\n2\n64.515219\n73.489190\n1.891025\n1.616301\n18.041283\n28.130585\n저체중\n비만\n큰키\n큰키\n\n\n3\n3\n96.048789\n56.877694\n1.831055\n1.723334\n28.647643\n19.151515\n비만\n정상\n큰키\n매우 큰키\n\n\n4\n4\n91.834611\n69.049447\n1.726567\n1.726978\n30.806303\n23.151891\n비만\n과체중\n보통\n매우 큰키\n\n\n\n\n\n\n\n\nh = df.melt(id_vars=[\"ID\"],\n            value_vars=[\"h_label_M\",\"h_label_F\"],\n            var_name = \"sex\",value_name='h')\n\nh.sex = h.sex.replace(\"h_label_M\",\"m\").replace(\"h_label_F\",\"f\")\nh.head()\n\n\n\n\n\n\n\n\nID\nsex\nh\n\n\n\n\n0\n0\nm\n작은키\n\n\n1\n1\nm\n보통\n\n\n2\n2\nm\n큰키\n\n\n3\n3\nm\n큰키\n\n\n4\n4\nm\n보통\n\n\n\n\n\n\n\n\ndf2 = pd.merge(df1,h, \n         left_on=['ID','sex'], right_on=['ID','sex'])\n\n\ndf2.head()\n\n\n\n\n\n\n\n\nID\nsex\nweight\nbmi\nh\n\n\n\n\n0\n0\nm\n69.401280\n26.269360\n작은키\n\n\n1\n1\nm\n96.582571\n31.357869\n보통\n\n\n2\n2\nm\n64.515219\n18.041283\n큰키\n\n\n3\n3\nm\n96.048789\n28.647643\n큰키\n\n\n4\n4\nm\n91.834611\n30.806303\n보통\n\n\n\n\n\n\n\n\nfig = df2.plot(x=\"weight\",y=\"bmi\",color=\"sex\",facet_col = \"h\",\n             facet_row=\"sex\",kind= \"scatter\",backend = \"plotly\",height=400, width=1000,opacity=0.2)\n\nfig.update_yaxes(matches=None)\nfig.show()"
  },
  {
    "objectID": "posts/study/2023-08-28-Extra 00. tidydata.html#summary-및-개인적인-생각",
    "href": "posts/study/2023-08-28-Extra 00. tidydata.html#summary-및-개인적인-생각",
    "title": "Extra 00. tidydata",
    "section": "summary 및 개인적인 생각",
    "text": "summary 및 개인적인 생각\n1. 물론 데이터를 처음 로드하고 탐색하기 위해 matplotlib를 이용하여 untidy한 데이터를 살펴본다.\n2. 그러나 후에 분석을 하고 그 결과를 시각화할 때는 tidy하지 못한 데이터로 할때 예쁜 그래프를 그리기 어렵다…(이건 개인적인 생각)\n3. 그리고 데이터를 3,4개의 차원(기준)에 따라 그래프를 그릴 때 untidy한 데이터로 어떻게 그릴지 접근조차 어렵다.(물론 가능한 분도 계시겠지만 거기까지 생각하고 싶지 않다.)\n4. 또한, 모델링 수행 수행시 tidy한 데이터가 들어가기 때문에 melt,groupby 등을 이용한 전처리는 필수적으로 갖추어야될 역량임.\n5. 이 부분은 스터디원 분들이 너무 겁먹지 않으셨으면 좋겠다. 하기 싫어도 매번 해야되서 언젠가 몸이 기억하고 반응할 거다."
  },
  {
    "objectID": "posts/study/2023-08-28-Extra 00. tidydata.html#ex2-전북대학교의-합격률-to-남녀-불평등",
    "href": "posts/study/2023-08-28-Extra 00. tidydata.html#ex2-전북대학교의-합격률-to-남녀-불평등",
    "title": "Extra 00. tidydata",
    "section": "ex2) 전북대학교의 합격률 \\(\\to\\) 남녀 불평등?",
    "text": "ex2) 전북대학교의 합격률 \\(\\to\\) 남녀 불평등?\n\n시각화 1 : 남녀 전체 합격률 비율 시각화\n- 아래와 같은 대학합격률 데이터가 있다고 하자.\n\nDEP : 학과\nSTATE : 합격 여부\nGEN : 성별\nCOUNT : 빈도\n\n\n\nCode\nDEP=(['A1']*2+['A2']*2+['B1']*2+['B2']*2)*2 \nGEN=['M']*8+['F']*8\nSTATE=['PASS','FAIL']*8\nCOUNT=[1,9,2,8,80,20,85,15,5,5,5,5,9,1,9,1]\n\ndf=pd.DataFrame({'DEP':DEP,'STATE':STATE,'GEN':GEN,'COUNT':COUNT})\ndf\n\n\n\n\n\n\n\n\n\nDEP\nSTATE\nGEN\nCOUNT\n\n\n\n\n0\nA1\nPASS\nM\n1\n\n\n1\nA1\nFAIL\nM\n9\n\n\n2\nA2\nPASS\nM\n2\n\n\n3\nA2\nFAIL\nM\n8\n\n\n4\nB1\nPASS\nM\n80\n\n\n5\nB1\nFAIL\nM\n20\n\n\n6\nB2\nPASS\nM\n85\n\n\n7\nB2\nFAIL\nM\n15\n\n\n8\nA1\nPASS\nF\n5\n\n\n9\nA1\nFAIL\nF\n5\n\n\n10\nA2\nPASS\nF\n5\n\n\n11\nA2\nFAIL\nF\n5\n\n\n12\nB1\nPASS\nF\n9\n\n\n13\nB1\nFAIL\nF\n1\n\n\n14\nB2\nPASS\nF\n9\n\n\n15\nB2\nFAIL\nF\n1\n\n\n\n\n\n\n\n\n\nCode\nt_g_c = df.groupby([\"GEN\",\"STATE\"],as_index=False)[[\"COUNT\"]].sum().\\\n            rename(columns = {\"COUNY\" : \"sum\"})\ngs = t_g_c.groupby(\"GEN\")[\"COUNT\"].sum()\ngs = list(np.repeat(gs,2))\nt_g_c[\"prop\"] = t_g_c[\"COUNT\"]/gs\nt_g_c\n\n\n\n\n\n\n\n\n\nGEN\nSTATE\nCOUNT\nprop\n\n\n\n\n0\nF\nFAIL\n12\n0.300000\n\n\n1\nF\nPASS\n28\n0.700000\n\n\n2\nM\nFAIL\n52\n0.236364\n\n\n3\nM\nPASS\n168\n0.763636\n\n\n\n\n\n\n\n\n\nCode\nfig = t_g_c.loc[t_g_c.STATE == \"PASS\"].\\\n        plot(x= \"GEN\", y=\"prop\",kind=\"bar\",\n            title=\"남녀 전북대학교의 합격률\",backend= \"plotly\",color=\"GEN\",\n             color_discrete_sequence=[\"red\",\"blue\"],height=500,width=700)\nfig.update_yaxes(range=[0.6, 0.8])\nfig.show()\n\n\n\n                                                \n\n\n- 전북대학교는 남녀 불평등인 학교인가?\n\n\n시각화 2 : (학과별, 성별) 합격률 비교\n\n\nCode\nt = df.groupby([\"DEP\",\"GEN\"],\n               as_index=False)[[\"COUNT\"]].sum().\\\n                rename(columns = {\"COUNT\":\"SUM\"}).merge(df)\nt[\"prop\"] = t.COUNT/t.SUM\nt\n\n\n\n\n\n\n\n\n\nDEP\nGEN\nSUM\nSTATE\nCOUNT\nprop\n\n\n\n\n0\nA1\nF\n10\nPASS\n5\n0.50\n\n\n1\nA1\nF\n10\nFAIL\n5\n0.50\n\n\n2\nA1\nM\n10\nPASS\n1\n0.10\n\n\n3\nA1\nM\n10\nFAIL\n9\n0.90\n\n\n4\nA2\nF\n10\nPASS\n5\n0.50\n\n\n5\nA2\nF\n10\nFAIL\n5\n0.50\n\n\n6\nA2\nM\n10\nPASS\n2\n0.20\n\n\n7\nA2\nM\n10\nFAIL\n8\n0.80\n\n\n8\nB1\nF\n10\nPASS\n9\n0.90\n\n\n9\nB1\nF\n10\nFAIL\n1\n0.10\n\n\n10\nB1\nM\n100\nPASS\n80\n0.80\n\n\n11\nB1\nM\n100\nFAIL\n20\n0.20\n\n\n12\nB2\nF\n10\nPASS\n9\n0.90\n\n\n13\nB2\nF\n10\nFAIL\n1\n0.10\n\n\n14\nB2\nM\n100\nPASS\n85\n0.85\n\n\n15\nB2\nM\n100\nFAIL\n15\n0.15\n\n\n\n\n\n\n\n\nfig = t.loc[t.STATE == \"PASS\",:].plot(x=\"GEN\", y=\"prop\", kind = \"bar\",backend = \"plotly\",\n          color= \"GEN\",color_discrete_sequence = [\"red\",\"blue\"],\n        facet_col=\"DEP\",facet_col_wrap=2,height=500,width=700)\n\nfig.update_yaxes(matches=None)\nfig.show()\n\n\n                                                \n\n\n- 실상은 그렇지 않다!\n\n\nsummary\n- 어떤 현상에 대해서 설명할 때 단순히 이분법적인 사고로 바라보면 안된다.\n\n위 예제처럼 잘못된 오해를 불러올 수 있는 상황이 발생할 수 있지 않은가?\n실제로 학과별 남녀 합격률을 조사한 결과 여학생의 비율이 더 많은 것을 확인했다.\n이 같은 현상을 심슨의 역설 이라고 하며 모델링 과정에서도 굉장히 중요한 부분을 차지한다."
  },
  {
    "objectID": "posts/2023-09-09-00. Linear Regression.html",
    "href": "posts/2023-09-09-00. Linear Regression.html",
    "title": "00. Linear Regression",
    "section": "",
    "text": "- 코랩환경에서 아래 순서대로 진행해야 ISLP 패키지 설치시 오류가 발생하지 않는다.\n\n드라이브 마운트\n현재 작업중인 경로로 이동\n패키지 import\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\ncd /content/drive/MyDrive/Colab Notebooks/ISLP/Linear Regression\n\n#pip install ISLP\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\"\n\nfrom ISLP import load_data\n\nimport statsmodels.api as sm\n\nfrom statsmodels.stats.outliers_influence \\\nimport variance_inflation_factor as VIF\nfrom statsmodels.stats.anova import anova_lm\nfrom ISLP.models import (ModelSpec as MS,\nsummarize,poly)"
  },
  {
    "objectID": "posts/2023-09-09-00. Linear Regression.html#표현1.-단순선형회귀",
    "href": "posts/2023-09-09-00. Linear Regression.html#표현1.-단순선형회귀",
    "title": "00. Linear Regression",
    "section": "표현1. 단순선형회귀",
    "text": "표현1. 단순선형회귀\n\\[\\hat {y} = \\hat {\\beta}_1x + \\hat {\\beta}_0\\]"
  },
  {
    "objectID": "posts/2023-09-09-00. Linear Regression.html#표현-2-starstarstar",
    "href": "posts/2023-09-09-00. Linear Regression.html#표현-2-starstarstar",
    "title": "00. Linear Regression",
    "section": "표현 2 (\\(\\star\\star\\star\\))",
    "text": "표현 2 (\\(\\star\\star\\star\\))\n- 표현 2가 자주 쓰이니 잘 알아두자.\n\\[\\bf \\hat Y = X \\hat {\\boldsymbol{\\beta}} = \\begin{bmatrix}\n1  & x_1 \\\\ 1 & x_2  \\\\ \\dots & \\dots \\\\  1  & x_n\\end{bmatrix}\\begin{bmatrix} \\hat \\beta_0  \\\\ \\hat \\beta_1 \\end{bmatrix} \\]"
  },
  {
    "objectID": "posts/2023-09-09-00. Linear Regression.html#잔차residual",
    "href": "posts/2023-09-09-00. Linear Regression.html#잔차residual",
    "title": "00. Linear Regression",
    "section": "잔차(residual)",
    "text": "잔차(residual)\n- \\(\\hat y_i = \\hat{\\beta_1} + \\hat {\\beta}_0x_i\\)는 개별 관측치 \\(x_i\\)가 주어졌을때 \\(i\\) 번째 \\(y\\)의 추정치이다.\n- 잔차는 표본으로부터 추정된 예측값에서 실제값의 차이로 다음과 같이 정의한다.\n\\[e_i = \\hat {y}_i - y_i,  \\quad e_i \\sim N(0,\\sigma^2)\\]\n- 그리고 책에서는 잔차제곱합을 다음과 같이 표현하니 눈에 익혀두자\n\\[RSS =\\sum_{i=1}^{n} e_i\\]"
  },
  {
    "objectID": "posts/2023-09-09-00. Linear Regression.html#회귀계수-추정-1.-단순선형회귀",
    "href": "posts/2023-09-09-00. Linear Regression.html#회귀계수-추정-1.-단순선형회귀",
    "title": "00. Linear Regression",
    "section": "회귀계수 추정 1. 단순선형회귀",
    "text": "회귀계수 추정 1. 단순선형회귀\n- 잔차제곱합을 최소로하는 \\((\\beta_0, \\beta_1)\\)은 다음과 같은 추정치로 구해진다.\n\\[\\hat{\\beta}_1= \\frac{\\sum (x-\\bar x)(y-\\bar  y)}{\\sum (x-\\bar x)^2}\\]\n\\[\\hat {\\beta}_0 = \\bar y - \\hat {\\beta}_1 \\bar x\\]"
  },
  {
    "objectID": "posts/2023-09-09-00. Linear Regression.html#회귀계수-추정-2.-일반적인-추정",
    "href": "posts/2023-09-09-00. Linear Regression.html#회귀계수-추정-2.-일반적인-추정",
    "title": "00. Linear Regression",
    "section": "회귀계수 추정 2. 일반적인 추정",
    "text": "회귀계수 추정 2. 일반적인 추정\n\\[\\boldsymbol{\\hat \\beta} = \\bf{(X^{\\top}X)^{-1}X^{\\top}y}\\]\n회귀계수 추정 및 벡터미분"
  },
  {
    "objectID": "posts/2023-09-09-00. Linear Regression.html#회귀계수의-신뢰구간",
    "href": "posts/2023-09-09-00. Linear Regression.html#회귀계수의-신뢰구간",
    "title": "00. Linear Regression",
    "section": "회귀계수의 신뢰구간",
    "text": "회귀계수의 신뢰구간\n\n표준오차\n- 정의 : 추정값인 표본평균들과 참값인 모평균과의 표준적인 차이\n- 보통 모집단의 평균 \\(\\mu\\)를 추정할 때 추정치의 표준오차는 다음과 같이 정의된다.\n\\[\\text {Var} (\\hat {\\mu}) = \\text{SE}(\\hat {\\mu}) ^2= \\frac {\\sigma^2}{ n}\\]\n- 여기서 \\(\\sigma\\)는 표본(\\(y_i\\))으로 부터 산출된 표준편차이다. (standard deviation)\n- 회귀계수의 표준편차\n\\[\\text{SE} (\\hat {\\beta}_0)^2 = \\sigma^2 \\left [ \\frac 1n + \\frac {\\bar x^2}{\\sum (x-x_i)^2}\\right],\\quad \\text{SE} (\\hat {\\beta}_0)^2  = \\frac {\\sigma^2}{\\sum (x-\\bar x)^2}\\]\n\n\n회귀계수의 유의성 검정\n- 유의수준 \\(\\alpha\\) 각 회귀계수의 유의성 가설은 다음과 같다.\n\\[H_0 : \\beta_i  = 0, \\quad H_1 : \\beta_i \\neq 0\\]\n- 이 가설에 대한 95% 신뢰구간은?\n\\[\\beta_i \\pm 1.96 \\cdot \\text {SE} (\\hat {\\beta}_i)\\]\n- 사용되는 검정통계량은 \\(t-statistic\\)이다.\n\\[t = \\frac {\\hat {\\beta}_i - 0}{\\text{SE}(\\hat {\\beta_i})} \\]\n\n\n모델평가\n1 RSE : estimation of the standard deviation of \\(\\varepsilon\\) : 잔차의 평균\n\n\\(p\\) : 예측변수의 개수\n\n\\[\\text{RSE} = \\sqrt {\\frac {1}{n-(p+1)} RSS}\\]\n2 결정 계수 : 적합한 모델이 데이터를 얼마나 잘 설명하는지 해석적 측면\n\\[R^2 = \\frac {\\text{TSS} - \\text{RSS}}{\\text{TSS}} =  1-\\frac{\\text{RSS}}{\\text{TSS}}\\]\n\\[R^2_{adj} = \\frac {\\text{TSS} - \\text{RSS)}\\,/\\,(n-(p+1)}{\\text{TSS}\\,/\\,(n-1)} =  1-\\frac{\\text{RSS}}{\\text{TSS}}\\]\n\\[\\text{TSS} = \\sum (y_i-\\bar y)^2\\]\n\n\n전체 모델의 유의성 검정\n- 가설 설정\n\\[H_0 : \\beta_1 = \\beta_1 = \\dots \\beta_p = 0, \\quad H_1  : \\text{not} \\,\\,H_0\\]\n- \\(F-statistic\\)\n\\[F = \\frac {(TSS - RSS)\\,/\\,p}{RSS\\,/\\,(n-(p+1))}\\]\n\n\nsummary\n- 정리하다보니 순서가 조금 엉켰지만! 회귀분석 순서는 아래의 순서로 기억하자.\n\n전체 모델에 대한 유의성 검정\n개별 변수에 대한 유의성 검정\n결정계수값 확인\n잔차분석 (이번 블로그에서는 위 내용은 생략)"
  },
  {
    "objectID": "posts/2023-09-09-00. Linear Regression.html#데이터-로드",
    "href": "posts/2023-09-09-00. Linear Regression.html#데이터-로드",
    "title": "00. Linear Regression",
    "section": "데이터 로드",
    "text": "데이터 로드\n- 아래의 데이터를 살펴보자.\n\nBoston data : 보스턴 시의 주택 가격에 대한 데이터 정보\n\ncrim : 자치시 별 1인당 범죄율\nzn : 25,000 평방피트르 초과하는 거주지역의 비율\nindus : 비소매상업지역이 점유하고 있는 토지의 비율\nchas : 찰스강에 대한 더미변수 (강의 경계에 위치한 경우는 1, 아니면 0)\nnos : 10ppm당 농축 일산화 질소\nrm : 주택 1가구당 평균 방의 개수\nage : 1940년 이전에 건축된 소유주택의 비율\ndis : 5개의 보스턴 직업센터까지의 접근성 지수\nrad : 방사형 도로까지의 접근성 지수\ntax : 10,000 달러 당 재산 세율\nptratio : 자치시별 학생/교사 비율\nlstat : 모집단의 하위 계층의 비율\nmedv : 본인 소유의 주택가격(중앙값) (단위 : $ 1,000)\n\n\n\nBoston = load_data(\"Boston\")\nBoston.columns\n#len(Boston.columns)\n\nIndex(['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'rad', 'tax',\n       'ptratio', 'lstat', 'medv'],\n      dtype='object')"
  },
  {
    "objectID": "posts/2023-09-09-00. Linear Regression.html#xy-생성",
    "href": "posts/2023-09-09-00. Linear Regression.html#xy-생성",
    "title": "00. Linear Regression",
    "section": "X,y 생성",
    "text": "X,y 생성\n\\[\\bf X = \\begin{bmatrix}\n1  & x_1 \\\\ 1 & x_2  \\\\ \\dots & \\dots \\\\  1  & x_n\\end{bmatrix}\\]\n\nlstat 변수를 예측변수로, medv변수를 반응변수로 사용\n\n\nX = pd.DataFrame({\"intercept\" : np.ones(Boston.shape[0]),\n                  \"lstat\" : Boston[\"lstat\"]})\ny = Boston[\"medv\"]\nX.head()\n\n\n\n\n\n\n\n\nintercept\nlstat\n\n\n\n\n0\n1.0\n4.98\n\n\n1\n1.0\n9.14\n\n\n2\n1.0\n4.03\n\n\n3\n1.0\n2.94\n\n\n4\n1.0\n5.33\n\n\n\n\n\n\n\n- OLS : Ordinary Least Squares의 약자로, 주어진 데이터에서 오차의 제곱을 최소화하는 \\(\\beta_i\\)를 추정한다."
  },
  {
    "objectID": "posts/2023-09-09-00. Linear Regression.html#모델-적합",
    "href": "posts/2023-09-09-00. Linear Regression.html#모델-적합",
    "title": "00. Linear Regression",
    "section": "모델 적합",
    "text": "모델 적합\n\nmodel = sm.OLS(y,X)\nresults = model.fit()\n\n\ns_result = summarize(results)\n\n\ns_result\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n\n\n\n\nintercept\n34.5538\n0.563\n61.415\n0.0\n\n\nlstat\n-0.9500\n0.039\n-24.528\n0.0\n\n\n\n\n\n\n\n- 적합된 모델을 해석하면 다음과 같다.\n\\[\\hat {\\text{medv}} = -0.95 \\times \\text{lstat} + 34\n.5538\\]"
  },
  {
    "objectID": "posts/2023-09-09-00. Linear Regression.html#시각화",
    "href": "posts/2023-09-09-00. Linear Regression.html#시각화",
    "title": "00. Linear Regression",
    "section": "시각화",
    "text": "시각화\n\nx = Boston[\"lstat\"]\nyhat = results.predict()\n\n\n\nCode\nplt.plot(x,y,\"o\",label = r\"$(x,y)$\",alpha=0.3)\nplt.plot(x,yhat,\"--\",label = r\"$(x,\\hat {y})$\")\nplt.legend()\n\n\n&lt;matplotlib.legend.Legend at 0x163b69b88d0&gt;"
  },
  {
    "objectID": "posts/2023-09-09-00. Linear Regression.html#modelspec-starstarstar",
    "href": "posts/2023-09-09-00. Linear Regression.html#modelspec-starstarstar",
    "title": "00. Linear Regression",
    "section": "ModelSpec() (\\(\\star\\star\\star\\))",
    "text": "ModelSpec() (\\(\\star\\star\\star\\))\n- 우리는 앞서 다음과 같은 module을 import 했다\nfrom ISLP.models import (ModelSpec as MS,\nsummarize,poly)\n- ModelSpec이라는 모듈을 MS로 사용할 것으로 지칭\n- 이 모듈은 예측변수 \\(x\\)를 컴퓨터가 이해할 수 있게끔 변환해준다.\n- step1. 전달할 \\(x\\)를 다음과 같이 전달\n\ndesign = MS(['lstat'])\n\n- step2. 컴퓨터가 이해할 수 있는 형태에 맞게 \\(x\\)를 변환 \\(\\to\\) 주어진 \\(x\\)를 매트릭스 형태로 변환해줌\n\nX = design.fit_transform(Boston)\n\n\nX.head()\n\n\n\n\n\n\n\n\nintercept\nlstat\n\n\n\n\n0\n1.0\n4.98\n\n\n1\n1.0\n9.14\n\n\n2\n1.0\n4.03\n\n\n3\n1.0\n2.94\n\n\n4\n1.0\n5.33\n\n\n\n\n\n\n\n\n모델 적합\n\nmodel = sm.OLS(y, X)\nresults = model.fit()\n\n\n\n모델 요약\n\nreport = results.summary()\n\n\nreport.tables[0]\n\n\nOLS Regression Results\n\n\nDep. Variable:\nmedv\nR-squared:\n0.544\n\n\nModel:\nOLS\nAdj. R-squared:\n0.543\n\n\nMethod:\nLeast Squares\nF-statistic:\n601.6\n\n\nDate:\nSat, 09 Sep 2023\nProb (F-statistic):\n5.08e-88\n\n\nTime:\n12:11:20\nLog-Likelihood:\n-1641.5\n\n\nNo. Observations:\n506\nAIC:\n3287.\n\n\nDf Residuals:\n504\nBIC:\n3295.\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n- 이런식으로 위의 표에 대한 데이터도 확인할 수 있음\n\nreport.tables[0].data\n\n[['Dep. Variable:', 'medv', '  R-squared:         ', '   0.544'],\n ['Model:', 'OLS', '  Adj. R-squared:    ', '   0.543'],\n ['Method:', 'Least Squares', '  F-statistic:       ', '   601.6'],\n ['Date:', 'Sat, 09 Sep 2023', '  Prob (F-statistic):', '5.08e-88'],\n ['Time:', '12:11:20', '  Log-Likelihood:    ', ' -1641.5'],\n ['No. Observations:', '   506', '  AIC:               ', '   3287.'],\n ['Df Residuals:', '   504', '  BIC:               ', '   3295.'],\n ['Df Model:', '     1', '                     ', ' '],\n ['Covariance Type:', 'nonrobust', '                     ', ' ']]\n\n\n- 추정된 회귀계수값 확인\n\nresults.params\n\nintercept    34.553841\nlstat        -0.950049\ndtype: float64\n\n\n\n\n모델 해석1 (모델의 유의성)\n\nreport.tables[0]\n\n\nOLS Regression Results\n\n\nDep. Variable:\nmedv\nR-squared:\n0.544\n\n\nModel:\nOLS\nAdj. R-squared:\n0.543\n\n\nMethod:\nLeast Squares\nF-statistic:\n601.6\n\n\nDate:\nSat, 09 Sep 2023\nProb (F-statistic):\n5.08e-88\n\n\nTime:\n12:11:20\nLog-Likelihood:\n-1641.5\n\n\nNo. Observations:\n506\nAIC:\n3287.\n\n\nDf Residuals:\n504\nBIC:\n3295.\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n- 결정계수 (\\(R^2\\)) 값을 살펴보니 약 54%의 설명력을 가진 모델이다.\n- 또한, F통계량에 근거한 p-value 값을 살펴보았을 때 주어진 표본으로 부터 추출된 모형은 통계적으로 유의하다.\n\n\n모델 해석2 (예측변수의 유의성)\n\nreport.tables[1]\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nintercept\n34.5538\n0.563\n61.415\n0.000\n33.448\n35.659\n\n\nlstat\n-0.9500\n0.039\n-24.528\n0.000\n-1.026\n-0.874\n\n\n\n\n\n- lstat의 coef \\(\\to\\) 즉, 회귀계수에 대한 유의성 검정결과\n\np-value값을 살펴본 결과 유의수준 0.05에서 유의성을 만족한다. 따라서 lstat의 회귀계수는 통계적으로 유의하다.\n\n\n\n시각화\n\nyhat = results.predict()\nx = X[\"lstat\"]\n\n- 적합된 \\((x,\\hat y)\\)와 \\((x,y)\\)를 시각화한 결과 몇몇 이상치를 제외하고 잘 예측하고 있는것 같다.\n\n\nCode\nplt.plot(x,y,\"o\",label = r\"$(x,y)$\",alpha=0.3)\nplt.plot(x,yhat,\"--\",label = r\"$(x,\\hat {y})$\")\nplt.legend()\n\n\n&lt;matplotlib.legend.Legend at 0x163b6c7cd10&gt;"
  },
  {
    "objectID": "posts/2023-09-09-00. Linear Regression.html#summary-lab",
    "href": "posts/2023-09-09-00. Linear Regression.html#summary-lab",
    "title": "00. Linear Regression",
    "section": "summary : Lab",
    "text": "summary : Lab\n- 구구절절 말이 많았지만, 회귀모형 적합 시 아래의 step을 기억하자.\n\n# step 0. import\nimport statsmodels.api as sm\nfrom ISLP.models import (ModelSpec as MS,\nsummarize,poly)\n\n# step 1. x, y를 저장\nx = data.columns.drop(\"target\") \ny = data[\"target\"]\n\n# step 2.  matrix 형태의 X 생성\nX = MS(final).fit_transform(data)\n\n# step 3. 최소제곱법(OLS)를 이용하여 모델을 적합하겠다고 선언\nmodel = sm.OLS(y,X)\n\n# step 4. 모형에 대한 유의성 확인\nmodel.fit().summary().tables[0]\n\n# step 5. 개별 회귀계수에 대한 유의성 확인\nmodel.fit().summary().tables[1]"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n- 전북대학교 통계학과 학사(부전공: 컴퓨터공학) 졸업 | 3.67 / 4.50 | 2015. 03 ~ 2021. 02\n- 전북대학교 통계학과 석사 졸업 | 4.44 / 4.50 | 2021. 03 ~ 2023. 02"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About Me",
    "section": "Experience",
    "text": "Experience\n- 국민연금공단 빅데이터부 현장실습 | 2020. 03 ~ 2020. 06\n- 지역 문화산업 융복합 데이터 전문가 과정 | 과학기술정보통신부, 한국데이터산업진흥원 | 2021. 06 ~ 2021. 08\n- 빅데이터 혁신공유대학사업 서포터즈 |전북대학교 빅데이터 현신공유대학사업| 2021. 07. 01 ~ 2021. 10. 31\n- KT AIVLE School DX Consultant Track | KT | 2023. 08 ~"
  },
  {
    "objectID": "about.html#publications",
    "href": "about.html#publications",
    "title": "About Me",
    "section": "Publications",
    "text": "Publications\n- 데이터 분석을 통한 지역별 고령친화도 시각화\n`-` 김영선, 강민구, 이강철 등  | 문화융복합아카이빙연구소 | 2021. 10 | 기록관리/보존 \n- 핵심어 추출 및 데이터 증강기법을 이용한 텍스트 분류 모델 성능 개선\n`-` 이강철, 안정용 | 한국자료분석학회 | 한국자료분석학회 | 2022. 10 | 통계학"
  },
  {
    "objectID": "about.html#certificate",
    "href": "about.html#certificate",
    "title": "About Me",
    "section": "Certificate",
    "text": "Certificate\n- 워드프로세서 | 대한상공회의소 | 19-19-017981 | 2019. 08. 30\n- 데이터분석준전문가(ADsP) | 한국데이터진흥원 | ADsP-0223898 | 2019. 10. 01\n- 사회조사분석사 2급 | 한국산업인력공단 | 19201142418N | 2019. 10. 01"
  },
  {
    "objectID": "about.html#conctact",
    "href": "about.html#conctact",
    "title": "About Me",
    "section": "Conctact",
    "text": "Conctact\n- rkdcjf8232@gmail.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISLP2023",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nSep 12, 2023\n\n\n01. tensorflow (1)\n\n\nstudy \n\n\n\n\nSep 10, 2023\n\n\n01. Classification\n\n\nGC \n\n\n\n\nSep 9, 2023\n\n\n00. Linear Regression\n\n\nGC \n\n\n\n\nSep 3, 2023\n\n\n00. 단순선형회귀분석\n\n\nGC \n\n\n\n\nAug 28, 2023\n\n\nExtra 00. tidydata\n\n\nGC \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-09-09-01. classification.html",
    "href": "posts/2023-09-09-01. classification.html",
    "title": "01. Classification",
    "section": "",
    "text": "- 해당 책에서는 우리가 평소에 회귀분석에서 쓰던 \\(\\text {MSE}= \\frac {SSE}{n-(p+1)}\\) 이라고 표현하지 않는다.\n- 교재에서 말하는 \\(\\text{MSE}\\)란 다음과 같다.\n\\[\\text{MSE} = E[(\\hat y - y)^2]\\]\n- 즉, 예측값과 실제값의 차이를 제곱합으로 계산 후 평균을 내는것이다.\n- 이를 다시 나누어 보자\n\\[\\begin {align} E[(\\hat y - y)^2] &= Var(\\hat y) + \\text {bias}^2   \\\\\n                                                                &= E[(\\hat y - E(\\hat y ))^2] + E[(E(\\hat y) - y)^2] + Var(\\varepsilon) \\end {align}\\]\n- \\(Var(\\hat y) \\, \\to \\,\\) 훈련자료의 변화에 \\(\\hat y\\)가 얼마나 민감하게 반응하는가?\n- \\(\\text {bias}^2\\) : 실제자료를 모형으로 얼마나 가깝게 근사할 수 있는가? (만약, \\(E(\\hat y) = y\\) 일 경우 해당 추정량은 불편추정량이다.)\n\n\\(Var(\\hat y),\\,\\text {bias}^2\\) 는 reducible error로 어떤 모델을 선택하느냐에 따라 줄일 수 있는 오차이다.\n\n- \\(Var(\\varepsilon)\\) : 어떤 모델을 사용하건 줄일 수 없는 오차\n- Flexibility : 모델 복잡도 측도\n\nFlexibility가 높아지면 \\(Var(\\hat y)\\) 는 높아지고, \\(\\text {bias}\\)는 낮아진다. (trade-off)\n이를 바꿔말하면 더 유연한(복잡한) 모형은 더 큰 분산을 가지고 더 단순한 모형일수록 큰 편의를 가진다.\n훈련 데이터로 모델 적합시, bias를 낮추기 위해 Var를 높여 모델 복잡도를 지나치게 높이는 것은 좋은 선택이 아니다.\n아래의 예시는 모형의 복잡도가 너무 높으면 평가 데이터에 대한 MSE가 오히려 과대추정되는 “과적합 문제” 를 보여주고 있다.\n\n\n\n\n\n\n\n\n\n- 실제로 위의 원데이터는 일반적인 선형모형으로 적합해도 괜찮을 것 같다.\n- 왼쪽그림에서 지나치게 복잡하게 적합된 초록색 선을 살펴보자.\n\n훈련 MSE(회색선) 는 3개의 모델 중 가장 낮으나, 과적합 이슈로 평가 MSE(빨간색선) 가 과대추정된 것을 볼 수 있다.\n\n\n\n\n\n\n\n- 위 데이터는 ex1과 달리 일반적인 선형회귀모형으로 적합하면 안될것 같은 느낌이 든다.\n- 파란색선은 중간 정도로 적합된 복잡한 모형이고, 초록색선은 지나치게 적합된 모형이다.\n\n오른쪽 그림을 살펴보면 파란색선으로 적합된 모형의 Test MSE가 가장 낮음을 볼 수 있다."
  },
  {
    "objectID": "posts/2023-09-09-01. classification.html#ex1",
    "href": "posts/2023-09-09-01. classification.html#ex1",
    "title": "01. Classification",
    "section": "",
    "text": "- 실제로 위의 원데이터는 일반적인 선형모형으로 적합해도 괜찮을 것 같다.\n- 왼쪽그림에서 지나치게 복잡하게 적합된 초록색 선을 살펴보자.\n\n훈련 MSE(회색선) 는 3개의 모델 중 가장 낮으나, 과적합 이슈로 평가 MSE(빨간색선) 가 과대추정된 것을 볼 수 있다."
  },
  {
    "objectID": "posts/2023-09-09-01. classification.html#ex2",
    "href": "posts/2023-09-09-01. classification.html#ex2",
    "title": "01. Classification",
    "section": "",
    "text": "- 위 데이터는 ex1과 달리 일반적인 선형회귀모형으로 적합하면 안될것 같은 느낌이 든다.\n- 파란색선은 중간 정도로 적합된 복잡한 모형이고, 초록색선은 지나치게 적합된 모형이다.\n\n오른쪽 그림을 살펴보면 파란색선으로 적합된 모형의 Test MSE가 가장 낮음을 볼 수 있다."
  },
  {
    "objectID": "posts/2023-09-09-01. classification.html#summary",
    "href": "posts/2023-09-09-01. classification.html#summary",
    "title": "01. Classification",
    "section": "summary",
    "text": "summary\n1 정확도 : 전체에서 예측값과 실제값이 일치하는 비율\n2 정밀도 : 내가 참이라고 예측한 것 중에 실제로 참인 비율 (전, 이걸 모델의 건방짐이라고 외웠습니다.)\n3 재현율 : 실제 참인 것들 중에서 모델이 참이라고 예측한 비율 (회사의 입장)"
  },
  {
    "objectID": "posts/2023-09-09-01. classification.html#로지스틱-회귀모형",
    "href": "posts/2023-09-09-01. classification.html#로지스틱-회귀모형",
    "title": "01. Classification",
    "section": "1. 로지스틱 회귀모형",
    "text": "1. 로지스틱 회귀모형\n- 로지스틱 모형은 target 변수인 \\(y\\)에 대한 직접적 모형화가 아닌 \\(y\\)가 특정 법주에 포함될 확률을 모형화한다.\n\\[P(y=1 \\,|\\, \\bf {X})\\]\n- 임계치(threshole)를 정하고 어떤 범주에 포함될 확률이 임계치보다 높으면 0 or 1로 예측하는 분류 모형이다. (이진분류에서!)\n\nmodel 1. 이진분류\nstep 1. \\(P(X)\\) : 0과 1사이의 값으로 예측해주는 모형화 작성 (로지스틱 함수)\n\\[P(X) = \\frac {\\exp (\\beta_0 + \\beta_{1}X)}{1+\\exp (\\beta_0 + \\beta_{1}X) } \\in  (0,1 )\\]\nstep 2. \\(\\frac{P(X)}{ 1- P(X)}\\) : odds, 배팅을 하는 분야에서 확률 대신 많이 쓰이는 측도\n\\[\\frac {P(X)}{ 1- P(X)} =  \\exp(\\beta_0 + \\beta_{1} X  ),\\quad  \\in (0,\\infty)\\]\nstep 3. \\(\\log \\frac {P(X)}{1-P(X)}\\) : 이와 같은 식을 logit이라고 하며, 해당 함수를 선형회귀처럼 작성하면 다음과 같다.\n\\[\\log \\frac {P(X)}{1-P(X)}  = \\beta_0 + \\beta_1 X,\\quad \\in (-\\infty, \\infty)\\]\n\n\n모델 추정(이진 분류)\n- 반응변수는 범주형 변수이나 모형을 통해 예측된 값은 확률이므로 최소제곱법 등은 정당화 되기 어려움\n\n최대우도추정법(maximum likelihood estimation)으로 보통 추정한다.\n반응변수가 베르누이 (혹은 아항) 분포임을 이용하여 가능도함수를 기술함\n\n\\[L = \\prod_{i : y_i=1}p(x_i) \\prod_{i^{\\prime} : y_i^{\\prime} =0}(1-p(x_i)) \\]\n\n\\((\\hat {\\beta}_0, \\hat {\\beta}_1)\\) : 위 수식을 최대화 하는 추정치이다.\n\n\n\nmodel 2. 다중분류\n\\[\\log \\frac {P(X)}{1-P(X)}  = \\beta_0 + \\beta_1 X + \\dots \\beta_{p}X_{p},\\quad \\in (-\\infty, \\infty)\\]\n\\[p(X) = \\frac { \\beta_0 + \\beta_1 X + \\dots \\beta_{p}X_{p}}{1+exp\\,( \\beta_0 + \\beta_1 X + \\dots \\beta_{p}X_{p})},\\quad \\in (0,1)\\]"
  },
  {
    "objectID": "posts/2023-09-09-01. classification.html#판별분석discriminant-analysis",
    "href": "posts/2023-09-09-01. classification.html#판별분석discriminant-analysis",
    "title": "01. Classification",
    "section": "2. 판별분석(discriminant analysis)",
    "text": "2. 판별분석(discriminant analysis)\n- 반응변수가 셋 이상의 범주를 가질 떄, 로지스틱 모형을 확장하여 위와 같이 고려할 수 있다.\n- 그러나 다른 대안들의 존재로 인해 이 경우 로지스틱 모형은 폭넓게 스이지는 않는다.\n- 대표적인 대안 중 하나는 선형판별분석이다.\n\n반응변수와 예측변수들의 결합분포에 기반한 방법\n\\(n\\)이작고 \\(X\\)의 분포가 정규분포에 가까울 때 판별분석의 성능이 로지스틱보다 좋다.\n\n- 베이즈 정리\n\n\\(k\\)번째 범주로부터 관측된 \\(X\\)의 분포를 \\(f_k(x)= P(X=x|Y =k) = P(X=x,Y =k)\\) 라 하고, \\(\\pi_k= P(Y=k)\\) 를 각 범주의 사전확률이라고 하면, 베이즈 정리에 의하여 다음을 얻는다.\n\n\\[P(Y=k | X=x) = \\frac{\\pi_k f_k(x)}{\\sum_{i=1}^{K} \\pi_{i}f_{i}(x)}\\]\n\n(1) LDA(Linear discriminant analysis)\n- \\(p=1, f_k(x)\\)가 정규분포임과 동시에 각 범주에 해당하는 분포의 분산은 동일하다 가정\n\n\\(p\\) 는 예측변수의 개수\n\n\\[X \\sim N(\\mu_k, \\sigma_k^{2})\\]\n- 조건부 확률 \\(P(Y=k | X=x))\\)를 최대화 시키는 \\(k\\)를 찾는 것은 아래의 판별함수를 최대화하는 \\(k\\)를 찾는 것이다.\n\n\\(\\hat{\\delta}_{k}\\) 를 최대화시키는 범주 \\(k\\)로 관측치를 할당.\n\n\\[\\delta_k(x) = x \\frac {\\mu_k}{\\sigma^2} - \\frac {\\mu_{k}^2}{2\\sigma^2}+ \\log \\pi_k\\]\n- \\(K=2\\)인 경우, 분류를 위한 경계치는\n\\[x = \\frac {\\mu_1+\\mu_2}{2}\\]\n\n\n\n- 또한, \\(p=1\\)인 경우, LDA는 다음과 같이 베이즈 분류기를 근사하게 된다.\n\\[\\hat {\\delta}_k(x) = x \\frac {\\hat {\\mu}_k}{\\hat{\\sigma}^2} - \\frac {\\hat{\\mu}_{k}^2}{2\\hat{\\sigma}^2}+ \\log \\hat{\\pi}_k\\]\n\\[\\hat {\\mu}_k = \\frac{1}{n_k} \\sum_{i : y_i=k} x_i\\]\n\\[\\hat {\\sigma}^2 = \\frac{1}{n-K} \\sum_{k=1}^K \\sum_{i:y_i = k} (x_i - \\hat {\\mu}_k)^2\\]\n\\[\\hat {P}(Y=k) = \\hat {\\pi}_k  = \\frac{n_k}{n}\\]\n- LDA의 linear라는 단어는 판별함수 \\(\\hat {\\delta}_k(x)\\)가 \\(x\\)의 선형함수로 표현된다는 사실로부터 유래한다.\n\nif, p&gt;1?\n- 예측변수가 다변량인 경우 \\(k\\)번째 범주에서 예측변수가 다변량 정규분포를 따른다는 가정으로부터 출발한다.\n\\[f_k(x) \\sim N(\\mu_k, \\Sigma)\\]\n- \\(p=1\\) 일 때와 마찬기지로 모든 범주에서 공분산 행렬 \\(\\sum\\)는 동일하다고 가정\n\\[\\delta_k(x) = x ^{\\top}{\\Sigma}^{-1} {\\mu_k}- \\frac {1}{2} \\mu_{k}^{\\top}{\\Sigma}^{-1}\\mu_k+ \\log \\pi_k\\]\n\n\n\n(2) QDA(Quadratic discriminant analysis)\n- LDA와 다르게 각 범주를 특정하는 정규분포의 분산에 이질성을 허용한다.\n\\[X \\sim N(\\mu_k, {\\Sigma}_k)\\]\n- 판별함수는 다음과 같다.\n\\[\\delta_k(x) = -\\frac{1}{2}(x-\\mu_k) ^{\\top}{\\Sigma}_{k}^{-1}(x-\\mu_k)- \\frac {1}{2}\\log |{\\Sigma}_k|\\log \\pi_k\\]"
  },
  {
    "objectID": "posts/2023-09-09-01. classification.html#knnk-nearest-neighbors",
    "href": "posts/2023-09-09-01. classification.html#knnk-nearest-neighbors",
    "title": "01. Classification",
    "section": "3. KNN(K-nearest neighbors)",
    "text": "3. KNN(K-nearest neighbors)\n- 조건부 확률을 인접한 \\(K\\)개의 data points의 상대비율로 추정\n\\[P(y=j | X = x_0) = \\frac{1}{K} \\sum_{i\\in N_0} I(y_i=j)\\]\n\\[N_0  : x_0  \\text{와 가장 가까운} K \\text{개의 자료의 집합}\\]\n- 위 확률을 최대로 하는 \\(j\\)로 관측치를 분류\n- K의 선택이 분류기의 성능을 결정하는데 매우 핵심적인 역할을 수행한다,"
  },
  {
    "objectID": "posts/2023-09-09-01. classification.html#summary-1",
    "href": "posts/2023-09-09-01. classification.html#summary-1",
    "title": "01. Classification",
    "section": "summary",
    "text": "summary\n- 범주가 2개일 때, LDA와 로지스틱모형은 선형적인 decision boundary를 생성한다는 측면에서 동일함\n- 각 범주의 분포가 정규분포로 잘 근사되는지 여부에 의해서 두 방식의 성능이 엇갈릴 수 있음\n- KNN은 decision boundary에 어떠한 가정도 하지 않음. 즉, decision boundary가 비선형인 경우 위 두 방식에 비해 우월성을 보임\n- QDA는 qudratic decision boundary를 설정한다는 면에서 KNN과 LDA 혹은 로지스틱의 중간쯤에 위치하는 방법으로 볼 수 있음\n\n모든 상황에서 항상 우월한 분류기는 없다"
  },
  {
    "objectID": "posts/2023-09-09-01. classification.html#naive-bayes",
    "href": "posts/2023-09-09-01. classification.html#naive-bayes",
    "title": "01. Classification",
    "section": "4. Naive Bayes",
    "text": "4. Naive Bayes\n- 사실 LDA, QDA의 경우 \\(X\\)의 확률분포가 정규분포라고 추정하는 것은 어렵다.\n\n예측변수 \\(X_j\\) 간의 어떠한 연관성이 있을수가 있음. 즉 독립이 아님\n따라서 추정이 어려움, 독립이 아닐경우, 결합분포, 주변확률 분포 등등 사전, 사후 확률 계산시 고려해야할 사항들이 너무 많고 복잡함.\n그래서 나이브 베이즈에서는 각각의 클래스안에서 예측변수들은 서로 독립이다. 라는 가정에서 출발\n\\(k\\)번째 클래스 안에서 \\(X_j\\)에 대한 확률분포는 다음과 같다.\n\n\\[X_j|Y = k \\sim N(\\mu_{jk},\\sigma^{2}_{jk})\\]\n- 아래는 \\(k\\)번째 클래스의 확률분포이다.\n\\[f_k(x) = f_{k1}(x_1) \\times \\dots  \\times f_{kp}(x_p) \\]\n- 이제 사후 확률을 계산식을 살펴보자\n\\[P(Y=k|X=x ) = \\frac{\\pi_k \\times f_{k1}(x_1) \\times f_{k2}(x_2) \\times f_{kp}(x_p)}{\\sum_{i=1}^{k}\\pi_i \\times f_{i1}(x_1) \\times f_{i2}(x_2) \\times f_{ip}(x_p) }\\]\n- example : \\(\\pi_1= \\pi_2= 0.5\\)\n$K=1, ({11}(0.4) = 0.368, ,{12}(1.5) = 0.484, , _{13}(1) = 0.226) $\n$K=2, ({21}(0.4) = 0.030, ,{22}(1.5) = 0.130, , _{23}(1) = 0.616) $\n위 공식을 이용하여 각각에 대한 사후활률을 추정하면?\n\nk1= 0.5*0.368*0.484*0.226/(0.5*0.368*0.484*0.226+ 0.5*0.030*0.130*0.616)\nk2 = 0.5*0.030*0.130*0.616/(0.5*0.368*0.484*0.226+ 0.5*0.030*0.130*0.616)\n\nprint(f\"P(y=1) ={k1 : .3f}, P(y=2) ={k2 : .3f} \")\n\nP(y=1) = 0.944, P(y=2) = 0.056 \n\n\n\n나이브 베이즈 분류기는 확실이 LDA, QDA보다 계산이 간단하고 모델링 시 더 빠르게 수행될 것 같음"
  },
  {
    "objectID": "posts/2023-09-09-01. classification.html#import",
    "href": "posts/2023-09-09-01. classification.html#import",
    "title": "01. Classification",
    "section": "import",
    "text": "import\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib.pyplot import subplots\nimport statsmodels.api as sm\nfrom ISLP import load_data\nfrom ISLP.models import (ModelSpec as MS,\nsummarize)\n\nfrom ISLP import confusion_table\nfrom ISLP.models import contrast\nfrom sklearn.discriminant_analysis import \\\n(LinearDiscriminantAnalysis as LDA,\nQuadraticDiscriminantAnalysis as QDA)\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\nimport plotly.express as px\nimport plotly.io as pio\npio.renderers.default = \"plotly_mimetype+notebook_connected\""
  },
  {
    "objectID": "posts/2023-09-09-01. classification.html#data-load",
    "href": "posts/2023-09-09-01. classification.html#data-load",
    "title": "01. Classification",
    "section": "data load",
    "text": "data load\n\nSmarket = load_data('Smarket')\nSmarket.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1250 entries, 0 to 1249\nData columns (total 9 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   Year       1250 non-null   int64  \n 1   Lag1       1250 non-null   float64\n 2   Lag2       1250 non-null   float64\n 3   Lag3       1250 non-null   float64\n 4   Lag4       1250 non-null   float64\n 5   Lag5       1250 non-null   float64\n 6   Volume     1250 non-null   float64\n 7   Today      1250 non-null   float64\n 8   Direction  1250 non-null   object \ndtypes: float64(7), int64(1), object(1)\nmemory usage: 88.0+ KB\n\n\n\nSmarket.head()\n\n\n\n\n\n\n\n\nYear\nLag1\nLag2\nLag3\nLag4\nLag5\nVolume\nToday\nDirection\n\n\n\n\n0\n2001\n0.381\n-0.192\n-2.624\n-1.055\n5.010\n1.1913\n0.959\nUp\n\n\n1\n2001\n0.959\n0.381\n-0.192\n-2.624\n-1.055\n1.2965\n1.032\nUp\n\n\n2\n2001\n1.032\n0.959\n0.381\n-0.192\n-2.624\n1.4112\n-0.623\nDown\n\n\n3\n2001\n-0.623\n1.032\n0.959\n0.381\n-0.192\n1.2760\n0.614\nUp\n\n\n4\n2001\n0.614\n-0.623\n1.032\n0.959\n0.381\n1.2057\n0.213\nUp\n\n\n\n\n\n\n\n- 해당 데이터는 2001년 부터 2005년까지 주식 데이터이다.\n\nYear는 해당 데이터가 관측된 년도\nLag1은 하루 전의 수익률\nLag2는 이틀 전의 수익률\n이렇게 Lag5까지 5일 전까지의 수익률을 가지고 있다.\nVolume은 하루에 평균적으로 몇 주가 거래되었는지\nToday는 오늘의 수익률\nDirection은 오늘 시장이 Up인지 Down인지 (target)"
  },
  {
    "objectID": "posts/2023-09-09-01. classification.html#correlation",
    "href": "posts/2023-09-09-01. classification.html#correlation",
    "title": "01. Classification",
    "section": "correlation",
    "text": "correlation\n\nSmarket.select_dtypes(\"number\").corr()\n\n\n\n\n\n\n\n\nYear\nLag1\nLag2\nLag3\nLag4\nLag5\nVolume\nToday\n\n\n\n\nYear\n1.000000\n0.029700\n0.030596\n0.033195\n0.035689\n0.029788\n0.539006\n0.030095\n\n\nLag1\n0.029700\n1.000000\n-0.026294\n-0.010803\n-0.002986\n-0.005675\n0.040910\n-0.026155\n\n\nLag2\n0.030596\n-0.026294\n1.000000\n-0.025897\n-0.010854\n-0.003558\n-0.043383\n-0.010250\n\n\nLag3\n0.033195\n-0.010803\n-0.025897\n1.000000\n-0.024051\n-0.018808\n-0.041824\n-0.002448\n\n\nLag4\n0.035689\n-0.002986\n-0.010854\n-0.024051\n1.000000\n-0.027084\n-0.048414\n-0.006900\n\n\nLag5\n0.029788\n-0.005675\n-0.003558\n-0.018808\n-0.027084\n1.000000\n-0.022002\n-0.034860\n\n\nVolume\n0.539006\n0.040910\n-0.043383\n-0.041824\n-0.048414\n-0.022002\n1.000000\n0.014592\n\n\nToday\n0.030095\n-0.026155\n-0.010250\n-0.002448\n-0.006900\n-0.034860\n0.014592\n1.000000\n\n\n\n\n\n\n\n\nLag_i, todayt 간 상관계수는 거의 0에 가깝다.\n유일하게 의미가 있아보이는 상관관계는 Year, Volume로 계산된 \\(r\\)이 ‘0.539’ 이다.\n다시 말해서, 2001년부터 2005년까지, 하루 평균 거래량이 증가하고 있다.\n\n\nSmarket.plot( y=\"Volume\")\n\n&lt;Axes: &gt;"
  },
  {
    "objectID": "posts/2023-09-09-01. classification.html#model1.-logistic",
    "href": "posts/2023-09-09-01. classification.html#model1.-logistic",
    "title": "01. Classification",
    "section": "model1. Logistic",
    "text": "model1. Logistic\n- Lag1 ~ Lag5 까지의 변수로 금일 시장이 up인지 down인지 예측해보자.\n\n# step 1. x,y를 저장\nallvars = Smarket.columns.drop([\"Today\",\"Year\",\"Direction\"])\n\nX = MS(allvars).fit_transform(Smarket) \ny = Smarket.Direction == \"Up\"\n\n# step 2.  모형 적합\n\nglm = sm.GLM(y,X , family = sm.families.Binomial())\n\n# step 3. 모형에 대한 유의성 확인\nresults = glm.fit()\nresults.pvalues\n\nintercept    0.600700\nLag1         0.145232\nLag2         0.398352\nLag3         0.824334\nLag4         0.851445\nLag5         0.834998\nVolume       0.392404\ndtype: float64\n\n\n\n유의미한 변수가 한개도 없다..\n그나마 p-value가 낮은, lag1, lag2만 포함시켜보자\n\n- predict() : 데이터 집합을 인자로 전달해주지 않으면 훈련 데이터를 이용하여 예측을 수행\n\n# step 1. x,y를 저장\n\nX = MS([\"Lag1\",\"Lag2\"]).fit_transform(Smarket) \ny = Smarket.Direction == \"Up\"\n\n# step 2.  모형 적합\n\nglm = sm.GLM(y,X , family = sm.families.Binomial())\n\n# step 3. 모형에 대한 유의성 확인\nresults = glm.fit()\n\n\nprobs = results.predict()\n\nlabels = np.array([\"Down\"]*1250)\nlabels[probs &gt; 0.5] = \"Up\"\nlabel = [\"Up\",\"Down\"]\n\nconfusion_table(labels, Smarket.Direction)\n\n\n\n\n\n\n\nTruth\nDown\nUp\n\n\nPredicted\n\n\n\n\n\n\nDown\n114\n102\n\n\nUp\n488\n546\n\n\n\n\n\n\n\n\n데이터셋 분할\n- 2005년 이전을 train, 이후를 test로 전환\n\ntrain = (Smarket.Year &lt; 2005)\nSmarket_train = Smarket.loc[train]\nSmarket_test = Smarket.loc[-train]\n\nX_train, X_test = X.loc[train], X.loc[-train]\ny_train, y_test = y.loc[train], y.loc[-train]\n\n\n\n모델 적합\n\nglm_train = sm.GLM(y_train,\n                  X_train,\n                family=sm.families.Binomial())\n\nresults = glm_train.fit()\nprobs = results.predict(exog=X_test)\nlabels = np.array(['Down']*252)\n\nlabels[probs &gt;0.5] = 'Up'\n\n\nD = Smarket.Direction\nL_train, L_test = D.loc[train], D.loc[-train]\n\n\nnp.mean(labels == L_test)\n\n0.5595238095238095\n\n\n\nreport = classification_report(y_true = L_test, y_pred = labels,target_names=label,output_dict=True)\nresult_acc_logistic = pd.DataFrame({\"acc\" : [report[\"accuracy\"]],\n                             \"model\" : [\"로지스틱\"]})\nresult_acc_logistic\n\n\n\n\n\n\n\n\nacc\nmodel\n\n\n\n\n0\n0.559524\n로지스틱\n\n\n\n\n\n\n\n\nresult_logistic = pd.DataFrame(report).\\\n                                    eval(\"model = '로지스틱'\")[[\"Up\",\"model\"]].\\\n                                                reset_index().iloc[:-1,]\nresult_logistic\n\n\n\n\n\n\n\n\nindex\nUp\nmodel\n\n\n\n\n0\nprecision\n0.500000\n로지스틱\n\n\n1\nrecall\n0.315315\n로지스틱\n\n\n2\nf1-score\n0.386740\n로지스틱"
  },
  {
    "objectID": "posts/study/2023-09-03-00. 단순선형회귀분석.html",
    "href": "posts/study/2023-09-03-00. 단순선형회귀분석.html",
    "title": "00. 단순선형회귀분석",
    "section": "",
    "text": "지도학습 : 우리가 무언가 예측하고자 하는 변수가 존재 (ex : 판매율, 주가 등등)\n\n회귀 : 예측하고자 하는 변수가 연속형 변수(오늘 소개하려는 단순선형회귀분석)\n분류 : 예측하고자 하는 변수가 범주형 변수(채무 이행, 불이행 문제)\n\n비지도학습 : 예측하고자 하는 타겟변수가 없고 주어진 변수들을 통해 성격이 유사한 녀석들끼리 그룹을 형성하게끔 만든다."
  },
  {
    "objectID": "posts/study/2023-09-03-00. 단순선형회귀분석.html#데이터-생성",
    "href": "posts/study/2023-09-03-00. 단순선형회귀분석.html#데이터-생성",
    "title": "00. 단순선형회귀분석",
    "section": "데이터 생성",
    "text": "데이터 생성\n\n\nCode\nx   = np.linspace(20,38,1000)\nepsilon = np.random.normal(size=1000)\ny  =  10.2 + 2.2*x + epsilon\nyhat = 10.2+2.2*x\nplt.plot(x,y,\".\",label = \"실제 데이터\",alpha=0.3)\nplt.plot(x,yhat,\".\",label = \"목표 모형\",alpha=0.3)\nplt.legend()\n\n\n&lt;matplotlib.legend.Legend at 0x19d7584a150&gt;"
  },
  {
    "objectID": "posts/study/2023-09-03-00. 단순선형회귀분석.html#최적의-beta를-구하는-방법-1",
    "href": "posts/study/2023-09-03-00. 단순선형회귀분석.html#최적의-beta를-구하는-방법-1",
    "title": "00. 단순선형회귀분석",
    "section": "최적의 \\(\\beta\\)를 구하는 방법 1",
    "text": "최적의 \\(\\beta\\)를 구하는 방법 1\n\n구하고자 하는 법칙\n베타 추정치\n\n\\[\\hat {\\beta}_1 = \\frac {\\sum (x-\\bar x) (y -\\bar y)} {\\sum (x-\\bar x)^2}= \\frac{S_{xy}}{S_{xx}}\\]\n\\[\\hat {\\beta}_0 = \\bar {y} - \\hat{\\beta}_1\\bar{x}\\]\n\n\nCode\nn = len(x)\nSxy = sum((x-np.mean(x))*(y-np.mean(y)) )\nSxx = sum((x-np.mean(x))**2)\n\nbeta1 = Sxy/Sxx\n\nbeta0 =  np.mean(y) - beta1*np.mean(x)\n\nplt.plot(x,y,\".\",label = \"실제 데이터\",alpha=0.15)\nplt.plot(x,yhat,\".\",label = \"목표 모형\",alpha=1.0)\nplt.plot(x,beta1*x+beta0,\".\",label = \"내가 만든 예측 모형\",alpha=0.1)\nplt.legend()\n\n\n&lt;matplotlib.legend.Legend at 0x19d76b11450&gt;\n\n\n\n\n\n\n구하고자 하는 법칙\n\n\\[\\hat{y} \\approx  2.2x + 10.2 \\]\n\nbeta1,beta0\n\n(2.201391988948589, 10.18755888066763)"
  },
  {
    "objectID": "posts/study/2023-09-03-00. 단순선형회귀분석.html#최적의-beta를-구하는-방법-2-starstarstar",
    "href": "posts/study/2023-09-03-00. 단순선형회귀분석.html#최적의-beta를-구하는-방법-2-starstarstar",
    "title": "00. 단순선형회귀분석",
    "section": "최적의 \\(\\beta\\)를 구하는 방법 2 (\\(\\star\\star\\star\\))",
    "text": "최적의 \\(\\beta\\)를 구하는 방법 2 (\\(\\star\\star\\star\\))\n- matrix 이용\n\\[\\hat {\\bf {\\beta}} = \\bf{(X^{\\top}X)^{-1}X^{\\top}Y}\\]\n\\[\\bf \\hat Y = X \\hat {\\boldsymbol{\\beta}} = \\begin{bmatrix}\n1  & x_{11}  & x_{12} &\\dots & x_{1p} \\\\ 1  & x_{21}  & x_{22} &\\dots & x_{2p}  \\\\ \\dots & \\dots \\\\  1  & x_{n1}  & x_{n2} &\\dots & x_{np}\\end{bmatrix}  \\,\\,\\begin{bmatrix} \\hat \\beta_0  \\\\ \\hat \\beta_1  \\\\ \\dots \\\\ \\beta_n \\end{bmatrix} \\]\n- X 생성\n\nX = pd.DataFrame({\"intercept\" :  np.ones(len(x)),\n                             \"x1\" : x})\n\nY = y.reshape(-1,1)\n\nbeta = np.linalg.inv(X.T @ X) @ X.T @Y\nbeta\n\n\n\n\n\n\n\n\n0\n\n\n\n\n0\n10.187559\n\n\n1\n2.201392\n\n\n\n\n\n\n\n\n\nCode\nplt.plot(x,y,\".\",label = \"실제 데이터\",alpha=0.06)\nplt.plot(x,yhat,\".\",label = \"목표 모형\",alpha=0.8)\nplt.plot(x,np.array(X) @ beta,\".\",label = \"내가 만든 예측 모형\",alpha=0.05)\nplt.legend()\n\n\n&lt;matplotlib.legend.Legend at 0x19d78d3da50&gt;"
  },
  {
    "objectID": "posts/study/2023-09-03-00. 단순선형회귀분석.html#예측-성능-평가",
    "href": "posts/study/2023-09-03-00. 단순선형회귀분석.html#예측-성능-평가",
    "title": "00. 단순선형회귀분석",
    "section": "예측 성능 평가",
    "text": "예측 성능 평가\n\nnp.mean((y-yhat)**2)\n\n0.9182134409556464"
  },
  {
    "objectID": "posts/study/2023-09-03-00. 단순선형회귀분석.html#step-1.-전달할-x를-다음과-같이-전달",
    "href": "posts/study/2023-09-03-00. 단순선형회귀분석.html#step-1.-전달할-x를-다음과-같이-전달",
    "title": "00. 단순선형회귀분석",
    "section": "step 1. 전달할 x를 다음과 같이 전달",
    "text": "step 1. 전달할 x를 다음과 같이 전달\n\ndesign = MS([\"x1\"])"
  },
  {
    "objectID": "posts/study/2023-09-03-00. 단순선형회귀분석.html#step-2.-컴퓨터가-이해할-수-있는-형태에-맞게-bfx를-생성",
    "href": "posts/study/2023-09-03-00. 단순선형회귀분석.html#step-2.-컴퓨터가-이해할-수-있는-형태에-맞게-bfx를-생성",
    "title": "00. 단순선형회귀분석",
    "section": "step 2. 컴퓨터가 이해할 수 있는 형태에 맞게 \\(\\bf{X}\\)를 생성",
    "text": "step 2. 컴퓨터가 이해할 수 있는 형태에 맞게 \\(\\bf{X}\\)를 생성\n\n_X = design.fit_transform(X)\n\n\n_X\n\n\n\n\n\n\n\n\nintercept\nx1\n\n\n\n\n0\n1.0\n20.000000\n\n\n1\n1.0\n20.018018\n\n\n2\n1.0\n20.036036\n\n\n3\n1.0\n20.054054\n\n\n4\n1.0\n20.072072\n\n\n...\n...\n...\n\n\n995\n1.0\n37.927928\n\n\n996\n1.0\n37.945946\n\n\n997\n1.0\n37.963964\n\n\n998\n1.0\n37.981982\n\n\n999\n1.0\n38.000000\n\n\n\n\n1000 rows × 2 columns"
  },
  {
    "objectID": "posts/study/2023-09-03-00. 단순선형회귀분석.html#step3.-모델-적합",
    "href": "posts/study/2023-09-03-00. 단순선형회귀분석.html#step3.-모델-적합",
    "title": "00. 단순선형회귀분석",
    "section": "step3. 모델 적합",
    "text": "step3. 모델 적합\n\nmodel = sm.OLS(y, _X)\nresults = model.fit()"
  },
  {
    "objectID": "posts/study/2023-09-03-00. 단순선형회귀분석.html#step4.-결과-확인",
    "href": "posts/study/2023-09-03-00. 단순선형회귀분석.html#step4.-결과-확인",
    "title": "00. 단순선형회귀분석",
    "section": "step4. 결과 확인",
    "text": "step4. 결과 확인\n\nsummarize(results)\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n\n\n\n\nintercept\n10.1876\n0.172\n59.320\n0.0\n\n\nx1\n2.2014\n0.006\n377.663\n0.0\n\n\n\n\n\n\n\n\nreport = results.summary()\nreport\n\n\nOLS Regression Results\n\n\nDep. Variable:\ny\nR-squared:\n0.993\n\n\nModel:\nOLS\nAdj. R-squared:\n0.993\n\n\nMethod:\nLeast Squares\nF-statistic:\n1.426e+05\n\n\nDate:\nTue, 05 Sep 2023\nProb (F-statistic):\n0.00\n\n\nTime:\n17:10:55\nLog-Likelihood:\n-1375.8\n\n\nNo. Observations:\n1000\nAIC:\n2756.\n\n\nDf Residuals:\n998\nBIC:\n2765.\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nintercept\n10.1876\n0.172\n59.320\n0.000\n9.851\n10.525\n\n\nx1\n2.2014\n0.006\n377.663\n0.000\n2.190\n2.213\n\n\n\n\n\n\nOmnibus:\n0.621\nDurbin-Watson:\n1.956\n\n\nProb(Omnibus):\n0.733\nJarque-Bera (JB):\n0.493\n\n\nSkew:\n0.004\nProb(JB):\n0.782\n\n\nKurtosis:\n3.108\nCond. No.\n167.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "posts/study/2023-09-03-00. 단순선형회귀분석.html#step5.-결과-시각화",
    "href": "posts/study/2023-09-03-00. 단순선형회귀분석.html#step5.-결과-시각화",
    "title": "00. 단순선형회귀분석",
    "section": "step5. 결과 시각화",
    "text": "step5. 결과 시각화\n\nx = _X[\"x1\"]\nyhat = results.predict()\n\n\nplt.plot(x,y,\"o\",label = r\"$(x,y)$\",alpha=0.3)\nplt.plot(x,yhat,\"--\",label = r\"$(x,\\hat {y})$\")\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x19d78ed9dd0&gt;"
  }
]